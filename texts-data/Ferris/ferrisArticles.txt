In a viral YouTube video from October 2011 a one-year-old girl sweeps her fingers across an iPad's touchscreen, shuffling groups of icons. In the following scenes she appears to pinch, swipe and prod the pages of paper magazines as though they too were screens. When nothing happens, she pushes against her leg, confirming that her finger works just fine—or so a title card would have us believe.

The girl's father, Jean-Louis Constanza, presents "A Magazine Is an iPad That Does Not Work" as naturalistic observation—a Jane Goodall among the chimps moment—that reveals a generational transition. "Technology codes our minds," he writes in the video's description. "Magazines are now useless and impossible to understand, for digital natives"—that is, for people who have been interacting with digital technologies from a very early age.

Perhaps his daughter really did expect the paper magazines to respond the same way an iPad would. Or maybe she had no expectations at all—maybe she just wanted to touch the magazines. Babies touch everything. Young children who have never seen a tablet like the iPad or an e-reader like the Kindle will still reach out and run their fingers across the pages of a paper book; they will jab at an illustration they like; heck, they will even taste the corner of a book. Today's so-called digital natives still interact with a mix of paper magazines and books, as well as tablets, smartphones and e-readers; using one kind of technology does not preclude them from understanding another.

Nevertheless, the video brings into focus an important question: How exactly does the technology we use to read change the way we read? How reading on screens differs from reading on paper is relevant not just to the youngest among us, but to just about everyone who reads—to anyone who routinely switches between working long hours in front of a computer at the office and leisurely reading paper magazines and books at home; to people who have embraced e-readers for their convenience and portability, but admit that for some reason they still prefer reading on paper; and to those who have already vowed to forgo tree pulp entirely. As digital texts and technologies become more prevalent, we gain new and more mobile ways of reading—but are we still reading as attentively and thoroughly? How do our brains respond differently to onscreen text than to words on paper? Should we be worried about dividing our attention between pixels and ink or is the validity of such concerns paper-thin?

Since at least the 1980s researchers in many different fields—including psychology, computer engineering, and library and information science—have investigated such questions in more than one hundred published studies. The matter is by no means settled. Before 1992 most studies concluded that people read slower, less accurately and less comprehensively on screens than on paper. Studies published since the early 1990s, however, have produced more inconsistent results: a slight majority has confirmed earlier conclusions, but almost as many have found few significant differences in reading speed or comprehension between paper and screens. And recent surveys suggest that although most people still prefer paper—especially when reading intensively—attitudes are changing as tablets and e-reading technology improve and reading digital books for facts and fun becomes more common. In the U.S., e-books currently make up between 15 and 20 percent of all trade book sales.

Even so, evidence from laboratory experiments, polls and consumer reports indicates that modern screens and e-readers fail to adequately recreate certain tactile experiences of reading on paper that many people miss and, more importantly, prevent people from navigating long texts in an intuitive and satisfying way. In turn, such navigational difficulties may subtly inhibit reading comprehension. Compared with paper, screens may also drain more of our mental resources while we are reading and make it a little harder to remember what we read when we are done. A parallel line of research focuses on people's attitudes toward different kinds of media. Whether they realize it or not, many people approach computers and tablets with a state of mind less conducive to learning than the one they bring to paper.
"There is physicality in reading," says developmental psychologist and cognitive scientist Maryanne Wolf of Tufts University, "maybe even more than we want to think about as we lurch into digital reading—as we move forward perhaps with too little reflection. I would like to preserve the absolute best of older forms, but know when to use the new."

Navigating textual landscapes
Understanding how reading on paper is different from reading on screens requires some explanation of how the brain interprets written language. We often think of reading as a cerebral activity concerned with the abstract—with thoughts and ideas, tone and themes, metaphors and motifs. As far as our brains are concerned, however, text is a tangible part of the physical world we inhabit. In fact, the brain essentially regards letters as physical objects because it does not really have another way of understanding them. As Wolf explains in her book Proust and the Squid, we are not born with brain circuits dedicated to reading. After all, we did not invent writing until relatively recently in our evolutionary history, around the fourth millennium B.C. So the human brain improvises a brand-new circuit for reading by weaving together various regions of neural tissue devoted to other abilities, such as spoken language, motor coordination and vision.

Some of these repurposed brain regions are specialized for object recognition—they are networks of neurons that help us instantly distinguish an apple from an orange, for example, yet classify both as fruit. Just as we learn that certain features—roundness, a twiggy stem, smooth skin—characterize an apple, we learn to recognize each letter by its particular arrangement of lines, curves and hollow spaces. Some of the earliest forms of writing, such as Sumerian cuneiform, began as characters shaped like the objects they represented—a person's head, an ear of barley, a fish. Some researchers see traces of these origins in modern alphabets: C as crescent moon, S as snake. Especially intricate characters—such as Chinese hanzi and Japanese kanji—activate motor regions in the brain involved in forming those characters on paper: The brain literally goes through the motions of writing when reading, even if the hands are empty. Researchers recently discovered that the same thing happens in a milder way when some people read cursive.

Beyond treating individual letters as physical objects, the human brain may also perceive a text in its entirety as a kind of physical landscape. When we read, we construct a mental representation of the text in which meaning is anchored to structure. The exact nature of such representations remains unclear, but they are likely similar to the mental maps we create of terrain—such as mountains and trails—and of man-made physical spaces, such as apartments and offices. Both anecdotally and in published studies, people report that when trying to locate a particular piece of written information they often remember where in the text it appeared. We might recall that we passed the red farmhouse near the start of the trail before we started climbing uphill through the forest; in a similar way, we remember that we read about Mr. Darcy rebuffing Elizabeth Bennett on the bottom of the left-hand page in one of the earlier chapters.

In most cases, paper books have more obvious topography than onscreen text. An open paperback presents a reader with two clearly defined domains—the left and right pages—and a total of eight corners with which to orient oneself. A reader can focus on a single page of a paper book without losing sight of the whole text: one can see where the book begins and ends and where one page is in relation to those borders. One can even feel the thickness of the pages read in one hand and pages to be read in the other. Turning the pages of a paper book is like leaving one footprint after another on the trail—there's a rhythm to it and a visible record of how far one has traveled. All these features not only make text in a paper book easily navigable, they also make it easier to form a coherent mental map of the text.

In contrast, most screens, e-readers, smartphones and tablets interfere with intuitive navigation of a text and inhibit people from mapping the journey in their minds. A reader of digital text might scroll through a seamless stream of words, tap forward one page at a time or use the search function to immediately locate a particular phrase—but it is difficult to see any one passage in the context of the entire text. As an analogy, imagine if Google Maps allowed people to navigate street by individual street, as well as to teleport to any specific address, but prevented them from zooming out to see a neighborhood, state or country. Although e-readers like the Kindle and tablets like the iPad re-create pagination—sometimes complete with page numbers, headers and illustrations—the screen only displays a single virtual page: it is there and then it is gone. Instead of hiking the trail yourself, the trees, rocks and moss move past you in flashes with no trace of what came before and no way to see what lies ahead.

"The implicit feel of where you are in a physical book turns out to be more important than we realized," says Abigail Sellen of Microsoft Research Cambridge in England and co-author of The Myth of the Paperless Office. "Only when you get an e-book do you start to miss it. I don't think e-book manufacturers have thought enough about how you might visualize where you are in a book."

At least a few studies suggest that by limiting the way people navigate texts, screens impair comprehension. In a study published in January 2013 Anne Mangen of the University of Stavanger in Norway and her colleagues asked 72 10th-grade students of similar reading ability to study one narrative and one expository text, each about 1,500 words in length. Half the students read the texts on paper and half read them in pdf files on computers with 15-inch liquid-crystal display (LCD) monitors. Afterward, students completed reading-comprehension tests consisting of multiple-choice and short-answer questions, during which they had access to the texts. Students who read the texts on computers performed a little worse than students who read on paper.

Based on observations during the study, Mangen thinks that students reading pdf files had a more difficult time finding particular information when referencing the texts. Volunteers on computers could only scroll or click through the pdfs one section at a time, whereas students reading on paper could hold the text in its entirety in their hands and quickly switch between different pages. Because of their easy navigability, paper books and documents may be better suited to absorption in a text. "The ease with which you can find out the beginning, end and everything inbetween and the constant connection to your path, your progress in the text, might be some way of making it less taxing cognitively, so you have more free capacity for comprehension," Mangen says.

Supporting this research, surveys indicate that screens and e-readers interfere with two other important aspects of navigating texts: serendipity and a sense of control. People report that they enjoy flipping to a previous section of a paper book when a sentence surfaces a memory of something they read earlier, for example, or quickly scanning ahead on a whim. People also like to have as much control over a text as possible—to highlight with chemical ink, easily write notes to themselves in the margins as well as deform the paper however they choose.

Because of these preferences—and because getting away from multipurpose screens improves concentration—people consistently say that when they really want to dive into a text, they read it on paper. In a 2011 survey of graduate students at National Taiwan University, the majority reported browsing a few paragraphs online before printing out the whole text for more in-depth reading. A 2008 survey of millennials (people born between 1980 and the early 2000s) at Salve Regina University in Rhode Island concluded that, "when it comes to reading a book, even they prefer good, old-fashioned print". And in a 2003 study conducted at the National Autonomous University of Mexico, nearly 80 percent of 687 surveyed students preferred to read text on paper as opposed to on a screen in order to "understand it with clarity".

Surveys and consumer reports also suggest that the sensory experiences typically associated with reading—especially tactile experiences—matter to people more than one might assume. Text on a computer, an e-reader and—somewhat ironically—on any touch-screen device is far more intangible than text on paper. Whereas a paper book is made from pages of printed letters fixed in a particular arrangement, the text that appears on a screen is not part of the device's hardware—it is an ephemeral image. When reading a paper book, one can feel the paper and ink and smooth or fold a page with one's fingers; the pages make a distinctive sound when turned; and underlining or highlighting a sentence with ink permanently alters the paper's chemistry. So far, digital texts have not satisfyingly replicated this kind of tactility (although some companies are innovating, at least with keyboards).

Paper books also have an immediately discernible size, shape and weight. We might refer to a hardcover edition of War and Peace as a hefty tome or a paperback Heart of Darkness as a slim volume. In contrast, although a digital text has a length—which is sometimes represented with a scroll or progress bar—it has no obvious shape or thickness. An e-reader always weighs the same, regardless of whether you are reading Proust's magnum opus or one of Hemingway's short stories. Some researchers have found that these discrepancies create enough "haptic dissonance" to dissuade some people from using e-readers. People expect books to look, feel and even smell a certain way; when they do not, reading sometimes becomes less enjoyable or even unpleasant. For others, the convenience of a slim portable e-reader outweighs any attachment they might have to the feel of paper books.

Exhaustive reading
Although many old and recent studies conclude that people understand what they read on paper more thoroughly than what they read on screens, the differences are often small. Some experiments, however, suggest that researchers should look not just at immediate reading comprehension, but also at long-term memory. In a 2003 study Kate Garland of the University of Leicester and her colleagues asked 50 British college students to read study material from an introductory economics course either on a computer monitor or in a spiral-bound booklet. After 20 minutes of reading Garland and her colleagues quizzed the students with multiple-choice questions. Students scored equally well regardless of the medium, but differed in how they remembered the information.

Psychologists distinguish between remembering something—which is to recall a piece of information along with contextual details, such as where, when and how one learned it—and knowing something, which is feeling that something is true without remembering how one learned the information. Generally, remembering is a weaker form of memory that is likely to fade unless it is converted into more stable, long-term memory that is "known" from then on. When taking the quiz, volunteers who had read study material on a monitor relied much more on remembering than on knowing, whereas students who read on paper depended equally on remembering and knowing. Garland and her colleagues think that students who read on paper learned the study material more thoroughly more quickly; they did not have to spend a lot of time searching their minds for information from the text, trying to trigger the right memory—they often just knew the answers.

Other researchers have suggested that people comprehend less when they read on a screen because screen-based reading is more physically and mentally taxing than reading on paper. E-ink is easy on the eyes because it reflects ambient light just like a paper book, but computer screens, smartphones and tablets like the iPad shine light directly into people's faces. Depending on the model of the device, glare, pixilation and flickers can also tire the eyes. LCDs are certainly gentler on eyes than their predecessor, cathode-ray tubes (CRT), but prolonged reading on glossy self-illuminated screens can cause eyestrain, headaches and blurred vision. Such symptoms are so common among people who read on screens—affecting around 70 percent of people who work long hours in front of computers—that the American Optometric Association officially recognizes computer vision syndrome.

Erik Wästlund of Karlstad University in Sweden has conducted some particularly rigorous research on whether paper or screens demand more physical and cognitive resources. In one of his experiments 72 volunteers completed the Higher Education Entrance Examination READ test—a 30-minute, Swedish-language reading-comprehension exam consisting of multiple-choice questions about five texts averaging 1,000 words each. People who took the test on a computer scored lower and reported higher levels of stress and tiredness than people who completed it on paper.

In another set of experiments 82 volunteers completed the READ test on computers, either as a paginated document or as a continuous piece of text. Afterward researchers assessed the students' attention and working memory, which is a collection of mental talents that allow people to temporarily store and manipulate information in their minds. Volunteers had to quickly close a series of pop-up windows, for example, sort virtual cards or remember digits that flashed on a screen. Like many cognitive abilities, working memory is a finite resource that diminishes with exertion.

Although people in both groups performed equally well on the READ test, those who had to scroll through the continuous text did not do as well on the attention and working-memory tests. Wästlund thinks that scrolling—which requires a reader to consciously focus on both the text and how they are moving it—drains more mental resources than turning or clicking a page, which are simpler and more automatic gestures. A 2004 study conducted at the University of Central Florida reached similar conclusions.

Attitude adjustments
An emerging collection of studies emphasizes that in addition to screens possibly taxing people's attention more than paper, people do not always bring as much mental effort to screens in the first place. Subconsciously, many people may think of reading on a computer or tablet as a less serious affair than reading on paper. Based on a detailed 2005 survey of 113 people in northern California, Ziming Liu of San Jose State University concluded that people reading on screens take a lot of shortcuts—they spend more time browsing, scanning and hunting for keywords compared with people reading on paper, and are more likely to read a document once, and only once.

When reading on screens, people seem less inclined to engage in what psychologists call metacognitive learning regulation—strategies such as setting specific goals, rereading difficult sections and checking how much one has understood along the way. In a 2011 experiment at the Technion–Israel Institute of Technology, college students took multiple-choice exams about expository texts either on computers or on paper. Researchers limited half the volunteers to a meager seven minutes of study time; the other half could review the text for as long as they liked. When under pressure to read quickly, students using computers and paper performed equally well. When managing their own study time, however, volunteers using paper scored about 10 percentage points higher. Presumably, students using paper approached the exam with a more studious frame of mind than their screen-reading peers, and more effectively directed their attention and working memory.

Perhaps, then, any discrepancies in reading comprehension between paper and screens will shrink as people's attitudes continue to change. The star of "A Magazine Is an iPad That Does Not Work" is three-and-a-half years old today and no longer interacts with paper magazines as though they were touchscreens, her father says. Perhaps she and her peers will grow up without the subtle bias against screens that seems to lurk in the minds of older generations. In current research for Microsoft, Sellen has learned that many people do not feel much ownership of e-books because of their impermanence and intangibility: "They think of using an e-book, not owning an e-book," she says. Participants in her studies say that when they really like an electronic book, they go out and get the paper version. This reminds Sellen of people's early opinions of digital music, which she has also studied. Despite initial resistance, people love curating, organizing and sharing digital music today. Attitudes toward e-books may transition in a similar way, especially if e-readers and tablets allow more sharing and social interaction than they currently do. Books on the Kindle can only be loaned once, for example.

To date, many engineers, designers and user-interface experts have worked hard to make reading on an e-reader or tablet as close to reading on paper as possible. E-ink resembles chemical ink and the simple layout of the Kindle's screen looks like a page in a paperback. Likewise, Apple's iBooks attempts to simulate the overall aesthetic of paper books, including somewhat realistic page-turning. Jaejeung Kim of KAIST Institute of Information Technology Convergence in South Korea and his colleagues have designed an innovative and unreleased interface that makes iBooks seem primitive. When using their interface, one can see the many individual pages one has read on the left side of the tablet and all the unread pages on the right side, as if holding a paperback in one's hands. A reader can also flip bundles of pages at a time with a flick of a finger.

But why, one could ask, are we working so hard to make reading with new technologies like tablets and e-readers so similar to the experience of reading on the very ancient technology that is paper? Why not keep paper and evolve screen-based reading into something else entirely? Screens obviously offer readers experiences that paper cannot. Scrolling may not be the ideal way to navigate a text aIn a viral YouTube video from October 2011 a one-year-old girl sweeps her fingers across an iPad's touchscreen, shuffling groups of icons. In the following scenes she appears to pinch, swipe and prod the pages of paper magazines as though they too were screens. When nothing happens, she pushes against her leg, confirming that her finger works just fine—or so a title card would have us believe.

The girl's father, Jean-Louis Constanza, presents "A Magazine Is an iPad That Does Not Work" as naturalistic observation—a Jane Goodall among the chimps moment—that reveals a generational transition. "Technology codes our minds," he writes in the video's description. "Magazines are now useless and impossible to understand, for digital natives"—that is, for people who have been interacting with digital technologies from a very early age.

Perhaps his daughter really did expect the paper magazines to respond the same way an iPad would. Or maybe she had no expectations at all—maybe she just wanted to touch the magazines. Babies touch everything. Young children who have never seen a tablet like the iPad or an e-reader like the Kindle will still reach out and run their fingers across the pages of a paper book; they will jab at an illustration they like; heck, they will even taste the corner of a book. Today's so-called digital natives still interact with a mix of paper magazines and books, as well as tablets, smartphones and e-readers; using one kind of technology does not preclude them from understanding another.

Nevertheless, the video brings into focus an important question: How exactly does the technology we use to read change the way we read? How reading on screens differs from reading on paper is relevant not just to the youngest among us, but to just about everyone who reads—to anyone who routinely switches between working long hours in front of a computer at the office and leisurely reading paper magazines and books at home; to people who have embraced e-readers for their convenience and portability, but admit that for some reason they still prefer reading on paper; and to those who have already vowed to forgo tree pulp entirely. As digital texts and technologies become more prevalent, we gain new and more mobile ways of reading—but are we still reading as attentively and thoroughly? How do our brains respond differently to onscreen text than to words on paper? Should we be worried about dividing our attention between pixels and ink or is the validity of such concerns paper-thin?

Since at least the 1980s researchers in many different fields—including psychology, computer engineering, and library and information science—have investigated such questions in more than one hundred published studies. The matter is by no means settled. Before 1992 most studies concluded that people read slower, less accurately and less comprehensively on screens than on paper. Studies published since the early 1990s, however, have produced more inconsistent results: a slight majority has confirmed earlier conclusions, but almost as many have found few significant differences in reading speed or comprehension between paper and screens. And recent surveys suggest that although most people still prefer paper—especially when reading intensively—attitudes are changing as tablets and e-reading technology improve and reading digital books for facts and fun becomes more common. In the U.S., e-books currently make up between 15 and 20 percent of all trade book sales.

Even so, evidence from laboratory experiments, polls and consumer reports indicates that modern screens and e-readers fail to adequately recreate certain tactile experiences of reading on paper that many people miss and, more importantly, prevent people from navigating long texts in an intuitive and satisfying way. In turn, such navigational difficulties may subtly inhibit reading comprehension. Compared with paper, screens may also drain more of our mental resources while we are reading and make it a little harder to remember what we read when we are done. A parallel line of research focuses on people's attitudes toward different kinds of media. Whether they realize it or not, many people approach computers and tablets with a state of mind less conducive to learning than the one they bring to paper.
"There is physicality in reading," says developmental psychologist and cognitive scientist Maryanne Wolf of Tufts University, "maybe even more than we want to think about as we lurch into digital reading—as we move forward perhaps with too little reflection. I would like to preserve the absolute best of older forms, but know when to use the new."

Navigating textual landscapes
Understanding how reading on paper is different from reading on screens requires some explanation of how the brain interprets written language. We often think of reading as a cerebral activity concerned with the abstract—with thoughts and ideas, tone and themes, metaphors and motifs. As far as our brains are concerned, however, text is a tangible part of the physical world we inhabit. In fact, the brain essentially regards letters as physical objects because it does not really have another way of understanding them. As Wolf explains in her book Proust and the Squid, we are not born with brain circuits dedicated to reading. After all, we did not invent writing until relatively recently in our evolutionary history, around the fourth millennium B.C. So the human brain improvises a brand-new circuit for reading by weaving together various regions of neural tissue devoted to other abilities, such as spoken language, motor coordination and vision.

Some of these repurposed brain regions are specialized for object recognition—they are networks of neurons that help us instantly distinguish an apple from an orange, for example, yet classify both as fruit. Just as we learn that certain features—roundness, a twiggy stem, smooth skin—characterize an apple, we learn to recognize each letter by its particular arrangement of lines, curves and hollow spaces. Some of the earliest forms of writing, such as Sumerian cuneiform, began as characters shaped like the objects they represented—a person's head, an ear of barley, a fish. Some researchers see traces of these origins in modern alphabets: C as crescent moon, S as snake. Especially intricate characters—such as Chinese hanzi and Japanese kanji—activate motor regions in the brain involved in forming those characters on paper: The brain literally goes through the motions of writing when reading, even if the hands are empty. Researchers recently discovered that the same thing happens in a milder way when some people read cursive.

Beyond treating individual letters as physical objects, the human brain may also perceive a text in its entirety as a kind of physical landscape. When we read, we construct a mental representation of the text in which meaning is anchored to structure. The exact nature of such representations remains unclear, but they are likely similar to the mental maps we create of terrain—such as mountains and trails—and of man-made physical spaces, such as apartments and offices. Both anecdotally and in published studies, people report that when trying to locate a particular piece of written information they often remember where in the text it appeared. We might recall that we passed the red farmhouse near the start of the trail before we started climbing uphill through the forest; in a similar way, we remember that we read about Mr. Darcy rebuffing Elizabeth Bennett on the bottom of the left-hand page in one of the earlier chapters.

In most cases, paper books have more obvious topography than onscreen text. An open paperback presents a reader with two clearly defined domains—the left and right pages—and a total of eight corners with which to orient oneself. A reader can focus on a single page of a paper book without losing sight of the whole text: one can see where the book begins and ends and where one page is in relation to those borders. One can even feel the thickness of the pages read in one hand and pages to be read in the other. Turning the pages of a paper book is like leaving one footprint after another on the trail—there's a rhythm to it and a visible record of how far one has traveled. All these features not only make text in a paper book easily navigable, they also make it easier to form a coherent mental map of the text.

In contrast, most screens, e-readers, smartphones and tablets interfere with intuitive navigation of a text and inhibit people from mapping the journey in their minds. A reader of digital text might scroll through a seamless stream of words, tap forward one page at a time or use the search function to immediately locate a particular phrase—but it is difficult to see any one passage in the context of the entire text. As an analogy, imagine if Google Maps allowed people to navigate street by individual street, as well as to teleport to any specific address, but prevented them from zooming out to see a neighborhood, state or country. Although e-readers like the Kindle and tablets like the iPad re-create pagination—sometimes complete with page numbers, headers and illustrations—the screen only displays a single virtual page: it is there and then it is gone. Instead of hiking the trail yourself, the trees, rocks and moss move past you in flashes with no trace of what came before and no way to see what lies ahead.

"The implicit feel of where you are in a physical book turns out to be more important than we realized," says Abigail Sellen of Microsoft Research Cambridge in England and co-author of The Myth of the Paperless Office. "Only when you get an e-book do you start to miss it. I don't think e-book manufacturers have thought enough about how you might visualize where you are in a book."

At least a few studies suggest that by limiting the way people navigate texts, screens impair comprehension. In a study published in January 2013 Anne Mangen of the University of Stavanger in Norway and her colleagues asked 72 10th-grade students of similar reading ability to study one narrative and one expository text, each about 1,500 words in length. Half the students read the texts on paper and half read them in pdf files on computers with 15-inch liquid-crystal display (LCD) monitors. Afterward, students completed reading-comprehension tests consisting of multiple-choice and short-answer questions, during which they had access to the texts. Students who read the texts on computers performed a little worse than students who read on paper.

Based on observations during the study, Mangen thinks that students reading pdf files had a more difficult time finding particular information when referencing the texts. Volunteers on computers could only scroll or click through the pdfs one section at a time, whereas students reading on paper could hold the text in its entirety in their hands and quickly switch between different pages. Because of their easy navigability, paper books and documents may be better suited to absorption in a text. "The ease with which you can find out the beginning, end and everything inbetween and the constant connection to your path, your progress in the text, might be some way of making it less taxing cognitively, so you have more free capacity for comprehension," Mangen says.

Supporting this research, surveys indicate that screens and e-readers interfere with two other important aspects of navigating texts: serendipity and a sense of control. People report that they enjoy flipping to a previous section of a paper book when a sentence surfaces a memory of something they read earlier, for example, or quickly scanning ahead on a whim. People also like to have as much control over a text as possible—to highlight with chemical ink, easily write notes to themselves in the margins as well as deform the paper however they choose.

Because of these preferences—and because getting away from multipurpose screens improves concentration—people consistently say that when they really want to dive into a text, they read it on paper. In a 2011 survey of graduate students at National Taiwan University, the majority reported browsing a few paragraphs online before printing out the whole text for more in-depth reading. A 2008 survey of millennials (people born between 1980 and the early 2000s) at Salve Regina University in Rhode Island concluded that, "when it comes to reading a book, even they prefer good, old-fashioned print". And in a 2003 study conducted at the National Autonomous University of Mexico, nearly 80 percent of 687 surveyed students preferred to read text on paper as opposed to on a screen in order to "understand it with clarity".

Surveys and consumer reports also suggest that the sensory experiences typically associated with reading—especially tactile experiences—matter to people more than one might assume. Text on a computer, an e-reader and—somewhat ironically—on any touch-screen device is far more intangible than text on paper. Whereas a paper book is made from pages of printed letters fixed in a particular arrangement, the text that appears on a screen is not part of the device's hardware—it is an ephemeral image. When reading a paper book, one can feel the paper and ink and smooth or fold a page with one's fingers; the pages make a distinctive sound when turned; and underlining or highlighting a sentence with ink permanently alters the paper's chemistry. So far, digital texts have not satisfyingly replicated this kind of tactility (although some companies are innovating, at least with keyboards).

Paper books also have an immediately discernible size, shape and weight. We might refer to a hardcover edition of War and Peace as a hefty tome or a paperback Heart of Darkness as a slim volume. In contrast, although a digital text has a length—which is sometimes represented with a scroll or progress bar—it has no obvious shape or thickness. An e-reader always weighs the same, regardless of whether you are reading Proust's magnum opus or one of Hemingway's short stories. Some researchers have found that these discrepancies create enough "haptic dissonance" to dissuade some people from using e-readers. People expect books to look, feel and even smell a certain way; when they do not, reading sometimes becomes less enjoyable or even unpleasant. For others, the convenience of a slim portable e-reader outweighs any attachment they might have to the feel of paper books.

Exhaustive reading
Although many old and recent studies conclude that people understand what they read on paper more thoroughly than what they read on screens, the differences are often small. Some experiments, however, suggest that researchers should look not just at immediate reading comprehension, but also at long-term memory. In a 2003 study Kate Garland of the University of Leicester and her colleagues asked 50 British college students to read study material from an introductory economics course either on a computer monitor or in a spiral-bound booklet. After 20 minutes of reading Garland and her colleagues quizzed the students with multiple-choice questions. Students scored equally well regardless of the medium, but differed in how they remembered the information.

Psychologists distinguish between remembering something—which is to recall a piece of information along with contextual details, such as where, when and how one learned it—and knowing something, which is feeling that something is true without remembering how one learned the information. Generally, remembering is a weaker form of memory that is likely to fade unless it is converted into more stable, long-term memory that is "known" from then on. When taking the quiz, volunteers who had read study material on a monitor relied much more on remembering than on knowing, whereas students who read on paper depended equally on remembering and knowing. Garland and her colleagues think that students who read on paper learned the study material more thoroughly more quickly; they did not have to spend a lot of time searching their minds for information from the text, trying to trigger the right memory—they often just knew the answers.

Other researchers have suggested that people comprehend less when they read on a screen because screen-based reading is more physically and mentally taxing than reading on paper. E-ink is easy on the eyes because it reflects ambient light just like a paper book, but computer screens, smartphones and tablets like the iPad shine light directly into people's faces. Depending on the model of the device, glare, pixilation and flickers can also tire the eyes. LCDs are certainly gentler on eyes than their predecessor, cathode-ray tubes (CRT), but prolonged reading on glossy self-illuminated screens can cause eyestrain, headaches and blurred vision. Such symptoms are so common among people who read on screens—affecting around 70 percent of people who work long hours in front of computers—that the American Optometric Association officially recognizes computer vision syndrome.

Erik Wästlund of Karlstad University in Sweden has conducted some particularly rigorous research on whether paper or screens demand more physical and cognitive resources. In one of his experiments 72 volunteers completed the Higher Education Entrance Examination READ test—a 30-minute, Swedish-language reading-comprehension exam consisting of multiple-choice questions about five texts averaging 1,000 words each. People who took the test on a computer scored lower and reported higher levels of stress and tiredness than people who completed it on paper.

In another set of experiments 82 volunteers completed the READ test on computers, either as a paginated document or as a continuous piece of text. Afterward researchers assessed the students' attention and working memory, which is a collection of mental talents that allow people to temporarily store and manipulate information in their minds. Volunteers had to quickly close a series of pop-up windows, for example, sort virtual cards or remember digits that flashed on a screen. Like many cognitive abilities, working memory is a finite resource that diminishes with exertion.

Although people in both groups performed equally well on the READ test, those who had to scroll through the continuous text did not do as well on the attention and working-memory tests. Wästlund thinks that scrolling—which requires a reader to consciously focus on both the text and how they are moving it—drains more mental resources than turning or clicking a page, which are simpler and more automatic gestures. A 2004 study conducted at the University of Central Florida reached similar conclusions.

Attitude adjustments
An emerging collection of studies emphasizes that in addition to screens possibly taxing people's attention more than paper, people do not always bring as much mental effort to screens in the first place. Subconsciously, many people may think of reading on a computer or tablet as a less serious affair than reading on paper. Based on a detailed 2005 survey of 113 people in northern California, Ziming Liu of San Jose State University concluded that people reading on screens take a lot of shortcuts—they spend more time browsing, scanning and hunting for keywords compared with people reading on paper, and are more likely to read a document once, and only once.

When reading on screens, people seem less inclined to engage in what psychologists call metacognitive learning regulation—strategies such as setting specific goals, rereading difficult sections and checking how much one has understood along the way. In a 2011 experiment at the Technion–Israel Institute of Technology, college students took multiple-choice exams about expository texts either on computers or on paper. Researchers limited half the volunteers to a meager seven minutes of study time; the other half could review the text for as long as they liked. When under pressure to read quickly, students using computers and paper performed equally well. When managing their own study time, however, volunteers using paper scored about 10 percentage points higher. Presumably, students using paper approached the exam with a more studious frame of mind than their screen-reading peers, and more effectively directed their attention and working memory.

Perhaps, then, any discrepancies in reading comprehension between paper and screens will shrink as people's attitudes continue to change. The star of "A Magazine Is an iPad That Does Not Work" is three-and-a-half years old today and no longer interacts with paper magazines as though they were touchscreens, her father says. Perhaps she and her peers will grow up without the subtle bias against screens that seems to lurk in the minds of older generations. In current research for Microsoft, Sellen has learned that many people do not feel much ownership of e-books because of their impermanence and intangibility: "They think of using an e-book, not owning an e-book," she says. Participants in her studies say that when they really like an electronic book, they go out and get the paper version. This reminds Sellen of people's early opinions of digital music, which she has also studied. Despite initial resistance, people love curating, organizing and sharing digital music today. Attitudes toward e-books may transition in a similar way, especially if e-readers and tablets allow more sharing and social interaction than they currently do. Books on the Kindle can only be loaned once, for example.

To date, many engineers, designers and user-interface experts have worked hard to make reading on an e-reader or tablet as close to reading on paper as possible. E-ink resembles chemical ink and the simple layout of the Kindle's screen looks like a page in a paperback. Likewise, Apple's iBooks attempts to simulate the overall aesthetic of paper books, including somewhat realistic page-turning. Jaejeung Kim of KAIST Institute of Information Technology Convergence in South Korea and his colleagues have designed an innovative and unreleased interface that makes iBooks seem primitive. When using their interface, one can see the many individual pages one has read on the left side of the tablet and all the unread pages on the right side, as if holding a paperback in one's hands. A reader can also flip bundles of pages at a time with a flick of a finger.

But why, one could ask, are we working so hard to make reading with new technologies like tablets and e-readers so similar to the experience of reading on the very ancient technology that is paper? Why not keep paper and evolve screen-based reading into something else entirely? Screens obviously offer readers experiences that paper cannot. Scrolling may not be the ideal way to navigate a text as long and dense as Moby Dick, but the New York Times, Washington Post, ESPN and other media outlets have created beautiful, highly visual articles that depend entirely on scrolling and could not appear in print in the same way. Some Web comics and infographics turn scrolling into a strength rather than a weakness. Similarly, Robin Sloan has pioneered the tap essay for mobile devices. The immensely popular interactive Scale of the Universe tool could not have been made on paper in any practical way. New e-publishing companies like Atavist offer tablet readers long-form journalism with embedded interactive graphics, maps, timelines, animations and sound tracks. And some writers are pairing up with computer programmers to produce ever more sophisticated interactive fiction and nonfiction in which one's choices determine what one reads, hears and sees next.

When it comes to intensively reading long pieces of plain text, paper and ink may still have the advantage. But text is not the only way to read.s long and dense as Moby Dick, but the New York Times, Washington Post, ESPN and other media outlets have created beautiful, highly visual articles that depend entirely on scrolling and could not appear in print in the same way. Some Web comics and infographics turn scrolling into a strength rather than a weakness. Similarly, Robin Sloan has pioneered the tap essay for mobile devices. The immensely popular interactive Scale of the Universe tool could not have been made on paper in any practical way. New e-publishing companies like Atavist offer tablet readers long-form journalism with embedded interactive graphics, maps, timelines, animations and sound tracks. And some writers are pairing up with computer programmers to produce ever more sophisticated interactive fiction and nonfiction in which one's choices determine what one reads, hears and sees next.

When it comes to intensively reading long pieces of plain text, paper and ink may still have the advantage. But text is not the only way to read.

Michael Utley does not remember much about his death. 
Over the years, he has woven together a narrative of what happened using threads collected from witnesses, friends, and family. On May 8, 2000, Utley, a 48-year-old stockbroker, was golfing with his coworkers Dick Gill and Bill Todd, along with their friend Jim Sullivan, in the village of Pocasset, Massachusetts, about three miles south of the Cape Cod Canal. Shortly after lunch, the dark clouds that had been mushrooming in the distance all morning were hovering close enough to merit the bleating of the course’s storm horn—time to clear the green.
Gill, Todd, and Sullivan immediately headed toward the clubhouse. Utley walked back to the hole and returned the flagstick. Seconds later, the guys in front heard a thunderous crack and turned to see Utley stumbling to the ground, tendrils of smoke curling off his body. Their friend had collapsed in a single perplexing instant. His shoes were several feet away from his body; his fingers looked like they had been flambéed; his eyebrows and wavy chestnut hair were wiry and crisped. Gill, an ex-Marine who had recently taken a refresher course in CPR, ran to Utley’s side, began blowing air into his lungs, and instructed Todd to perform chest compressions. As Sullivan rushed off to get help, the clouds unleashed a deluge of rain and hail.
Utley cannot recall any of this. Not the arrival of the paramedics, nor having his heart restarted in the ambulance on the way to the hospital. His first memory after leaving the golf course is of waking up in a different ambulance, tubes down his throat, monitors everywhere, and a paramedic in a blue smock at his feet.
“Where am I?” Utley rasped.
“You’re on your way to rehab,” the paramedic said.
“What the fuck happened?”
“You were struck by lightning 38 days ago.”

In popular culture, to be hit by a bolt of lightning is to suffer extremely bad luck. Rain, snow, and hail are largely indiscriminate: within a certain radius, everything is drenched, blanketed, or pelted. A cloud-to-ground lightning bolt is different. It blazes a discrete path through the sky. It appears to have choice. When lightning hits a human being, a survivor must reconcile not only what happened but why it happened. Why me? For most victims, it is not the unforgettable horror of an agonizing ordeal that haunts them—many can’t even recall the incident itself; it’s the mysterious physical and psychological symptoms that emerge, often long after their immediate wounds have healed and doctors have cleared them to return to their normal routines. But nothing is normal anymore. Chronic pain, memory trouble, personality changes, and mood swings can all follow an encounter with lightning, leaving friends and family members confused, while survivors, grappling with a fundamental shift in identity, feel increasingly alienated by the incomprehensible nature of their condition. Something happened in a single moment—something strange and rare, something unbelievable—and after that moment, everything has changed.
Even more confounding is that almost no one in the mainstream medical community can explain what’s happening to them. Although many scientists have spent their careers examining the physics of lightning, only a handful of doctors and researchers have devoted themselves to the study of how lightning damages the human body. The incident rates are simply not high enough to warrant an entire subfield of science. Nearly everything we now know about treating lightning victims concerns the immediate wounds, many of which don’t even require special medical knowledge.
Paramedics, often needing to treat victims who aren’t entirely sure what has happened to them, receive brief training on how to recognize the common signs of a lightning strike. True entry and exit wounds are uncommon, but lightning typically leaves some kind of mark on the skin. One afternoon in 2009, a hiker named Becky Garriss awoke on the Appalachian Trail in Vermont, sitting on a bed of pine needles, her back against a tree, as though she’d fallen asleep in its shade. Her right arm was paralyzed, pinned against her chest in a pledge of allegiance. Here and there, her pants were charred. Although she was disoriented and scared, she managed to hike more than ten muddy miles down Glastenbury Mountain to call for help. When she got to a hospital, doctors recognized lightning’s smoldering touch on Garriss’s right arm and leg. A bolt probably hit her directly, they told her.

Other survivors awaken into temporary blindness or deafness; sometimes the concussive force of the strike—or the electricity itself—ruptures eardrums. Some victims report the taste of metal on their tongues. Now and then, survivors develop strangely beautiful pink and brown bruises known as Lichtenburg figures, which look like intricate henna tattoos of branching fronds. These bruises likely trace the path of electricity that forced blood cells out of capillaries into more superficial layers of skin.
In rare instances, the surge of electricity is enough to stop a victim’s heart and lungs. That’s what happened to Michael Utley. But cardiac arrest is something any paramedic knows how to handle. Twenty minutes after Utley was struck, EMTs had arrived on the scene, strapped him to a gurney, and loaded him into an ambulance. They used a defibrillator to keep his heart going. Doctors at Boston’s Brigham and Women’s Hospital then spent more than five weeks caring for Utley before they determined that he was ready for rehabilitation.
After leaving the hospital, Utley spent months relearning to swallow, move his fingers, and walk. Rehab was just the first chapter of his ordeal, however. In his previous life, Utley was a successful stockbroker who often went skiing and windsurfing. Today, at 62, he lives on disability insurance in Cape Cod. “I don’t work,” he says. “I can’t work. My memory’s fried, and I don’t have energy like I used to. I aged 30 years in a second. I walk and talk and play golf—but I still fall down. I’m in pain most of the time. I can’t walk 100 yards without stopping. I look like a drunk.”
Lightning also dramatically altered his personality. “It made me a mean, ornery son of a bitch. I’m short-tempered. Nothing is fun anymore. I am just not the same person my wife married,” says Utley, who is now divorced. Like many survivors, Utley sees his fateful union with lightning as more than just a close call he was lucky to survive. It marks a moment in which he was split from himself.
On a typical summer afternoon, thunder-clouds above the continental United States generate an average of 50,000 lightning flashes per hour. Two-thirds of these stay near the heavens. They pierce the sky with branching networks of blue and white fire, or strike out a short distance in thin tongues of electricity, or illuminate clouds from within like muffled firecrackers. The remaining minority of lightning bolts, however, find earthbound targets—a church steeple, a telephone pole, a tree.
Even rarer are bolts that directly strike and kill humans. Not surprisingly, the vast majority of these fatalities in the U.S. happen in June, July, and August, the months when thunderstorms are more prevalent and the greatest number of Americans are recreating outside. According to a recent National Weather Service analysis, fishing, boating, swimming, and camping put the most people at risk each year. Last July, two visitors in Colorado’s Rocky Mountain National Park were killed by separate strikes on the same weekend.
When people and lightning meet, however, death is an unlikely outcome. Roy Cleveland, a ranger at Shenandoah National Park, in Virginia, survived a record seven strikes between 1942 and 1977. This fact appears to defy logic. An average lightning bolt carries 500 megajoules of energy—enough to instantly boil 250 gallons of water. It heats the air it zips through to five times the surface temperature of the sun. Still, around 90 percent of lightning-strike victims survive. Over the past three decades, lightning has killed an average of 51 people per year in the U.S. but left more than 500 injured and alive.
 Becky Elizabeth Garris struck by lightning
Lightning survivor Becky Garris. (Ethan Hill / REDUX Pictures)
One explanation is that lightning strikes are fundamentally different from the more common high-voltage electrical accidents in the home or workplace that people mistakenly compare them to. When an electrician inadvertently grabs a live wire, far less current typically seizes him than is contained in a lightning bolt, but it does so for a longer duration. The surge of current causes victims to lose control, rendering them unable to let go. After a few seconds, the electricity coursing through the body has enough time to sear internal organs and interrupt the heart. Lightning strikes, lasting less than a half-millionth of a second, often scorch the skin but don’t cause internal burns.
Just as crucial, most of the electricity in a lightning bolt does not pass through the body. Rather, it dissipates over the skin in what’s known as a flashover. Vernon Cooray, a lightning scientist at Uppsala University in Sweden, explains the phenomenon by contrasting the ways a human body and a tree react when struck. Both trees and people are filled with a soup of water and minerals that conduct electricity pretty well. But because trees are covered in dry, inelastic bark, lightning traveling through the trunk has no escape route. It must stay its course. In the process, it superheats the water and sap inside the tree into explosive steam, which can rip apart the trunk and branches.
Compared with tree bark, human skin is much more pliant and moist. Sweat and rainwater make it extra conductive, providing an alternate external path for voltage. Most of the electricity can pass over strike victims rather than coursing through them. “The path through the body has much greater resistance than the path around the body,” says Vladimir Rakov, a University of Florida researcher and one of the world’s leading authorities on lightning physics. “Current always chooses the path of least resistance.”
A flashover can still do damage indirectly. The electricity crackling over the surface of the human body singes clothing, vaporizes sweat and moisture into scalding steam, and renders metal objects like belt buckles, keys, and jewelry so hot that they burn the skin. Occasionally, all that steam even blows victims’ shoes and socks off.
The best advice for people who find themselves outside during a lightning storm is simply to get inside, either a home or a vehicle. Yet even buildings aren’t completely impervious to lightning strikes. You’ll want to stay off the telephone, out of the shower, and away from sinks. Lightning can pass through landlines, plumbing—metal pipes and faucets—and all manner of electrical wiring. Last February, it ruptured gas pipes in the crawl space of a house in Steuben County, Indiana. A kitchen appliance then ignited the vented gas, causing a massive explosion. The only family member home at the time was the dog, Boomer. A neighbor rescued him from the rubble after he was sent flying from the house in his crate.
One common type of lightning encounter, responsible for 20 to 30 percent of injuries, is a side flash or splash, when lightning leaps from one grounded object to another—from a building to a person, from a tree to a horse, or even from a person to another. In nearly all these incidents, too little electricity enters the body to be lethal. A direct strike almost always delivers more current inside a person, making it much more deadly. A strike like the one Utley suffered probably should have killed him, too. Had his friends not performed CPR so quickly, he wouldn’t be alive today.
For Utley, getting adequate treatment after he recovered was a struggle. He was eventually fortunate enough to find a few doctors who helped him cope with the long-term symptoms, but along the way he met many medical experts who understood little or nothing about the kind of injuries  he sustained.
“Finding a doctor who knows anything about a lightning strike is next to impossible,” says Tamara Pandolph-Peary, 46, who was struck by lightning in August 2010, in the parking lot of the Springfield, Illinois, Men’s Warehouse where she worked.
Following her accident, Pandolph-Peary forgot how to use everyday objects, like a potato peeler; she could no longer get from point A to point B in her hometown; she suffered migraines and fatigue; she tripped over her sentences or suddenly lost the ability to understand what other people were saying; she was often dizzy and off-balance; she had tremors and chronic pain, and would unpredictably lose control of various body parts; and every now and then, when her nerves were on fire, even the slightest touch was painfully intense.
“I struggled with the ‘Why me?’ initially,” she says. “There was a time I was angry. There was a time I really missed who I used to be. I think I got past that part. You can be angry and hold onto that, and it can ruin everything you have left.”
Mary Ann Cooper, professor emerita at the University of Illinois at Chicago, is one of the few medical doctors who have attempted to investigate how lightning alters the brain’s circuitry. A no-nonsense, bespectacled woman with a short-cropped bob of silvering strawberry hair, her fascination with lightning dates to childhood. “My dad swore his kids were not going to hide from thunderstorms in the closet or under the beds,” Cooper, now 65, recalls. “It was like the Fourth of July for us whenever we had a thunderstorm. We always watched them.”
In the seventies, a friend of a family member suffered a high-voltage electrical injury. Knowing that she was about to start medical school, Cooper’s friends started asking a lot of questions about how electricity harms the body and what to do about it. She began to investigate, and later, while still in school, she started lecturing about the burns people suffer due to industrial electrical accidents. At one talk, a member of the audience asked about lightning injuries. Cooper looked for relevant information in emergency medical textbooks but found nothing, so she decided to fill the gap herself.
Over the past three decades, Cooper has written articles on lightning safety, helped set up websites for survivors, and published many academic papers. A link on her UIC page points visitors to most of her work on the topic, including studies with esoteric titles such as “Electron Paramagnetic Resonance Spectroscopic Evidence of Increased Free Radical Generation and Selective Damage to Skeletal Muscle Following Lightning Injury.” Acquiring the funds and lab space for controlled experiments has been difficult. Much of Cooper’s work is based on observations, medical examinations, and mathematical models. Survivors often get in touch with her, and she has interviewed many of them in detail, looking for clues to how lightning wreaks its peculiar form of havoc.
Her typical case study might be someone like Phil Broscovak. In August 2005, Broscovak, his wife, their three young children, and Broscovak’s nephew and his girlfriend went camping in Vedauwoo, Wyoming, where ancient granite burgeons and spires from the earth. On August 13, after a campsite breakfast, the group decided to climb Edward’s Crack—a long, vertical fracture in a giant slab known as Walt’s Wall. There was nothing ominous in the skies when they began their ascent, merely a few scattered clouds. Shortly before noon, however, claps of thunder echoed in the distance, and a gray veil of unfriendly weather descended. By the time Broscovak managed to get everyone but himself back on the ground, it was raining and the thunder sounded much closer. He was in the process of retrieving gear from the wall of rock when his rope tangled in a bush. Oh God, he thought, scrambling across the slippery granite. It’s Ben Franklin all over again. After Broscovak ripped out the shrub by its roots, his rope got caught a second time, in a small crevice. While he was trying to undo the knot, it happened.
An immense blast of light. A sound like a grenade exploding in his head. A pain like a thousand wasps stinging him from within. A gelatinous blue plasma enveloping his body. And his leg jerking away as though pulled by invisible marionette strings. This is what Phil Broscovak remembers.

The lightning bolt hit Walt’s Wall just a few feet from Broscovak, splashed into his leg, and surged over his body, possibly exiting through a shoulder blade. The shock flung him from the rock and briefly knocked him unconscious. He awoke, dangling from his ropes and harness, to the screams of his terrified family 170 feet below. Spotting a patch of scorched rock beside him, he recalled the light and pain and realized what must have happened.
“I’m OK! I’m OK!” he yelled to his family. All he could think about was getting down to them as fast as he could. When he did, they rushed back to the car to wait out the storm. Although several people insisted that Broscovak go to the hospital, he didn’t think it was necessary. “My wife was the kind of person who would go to the doctor at the drop of a hat,” Broscovak says. “I’m the kind of person who has to have ribs poking out of my chest. I really did not take it seriously at the time.”
The next morning, however, he couldn’t stand straight. Any movement was painful. Every hair follicle seemed to ache. Far more troubling than the pain and soreness was the dramatic fluctuation of his mental acuity. Ever since the strike, Broscovak has slipped in and out of what he describes as fugue states. When the mists descend, he has trouble remembering even the simplest facts. Sitting at his computer, he’ll think one word and write another or will be incapable of understanding what he just typed on the screen. “I consider myself a very articulate person, but on a couple occasions I broke down in tears because I couldn’t remember how to spell the word the,” Broscovak tells me. He has struggled with insomnia, become hypersensitive to everyday sounds, and suffered from tinnitus. “I would rage and scream and stomp my feet. It was irrational, and no one would understand it. Those fugue states were a contributing factor to the end of my marriage.”
Broscovak, like many survivors, has also endured symptoms that are remarkably similar to those of post-traumatic stress disorder. Once, during a rainstorm after his divorce, he pulled into his driveway, called his roommate from the car, and insisted that she open the front door of their house so that he could dash inside as quickly as possible. Two years after the strike, while climbing Taylor Canyon in Colorado on another family camping trip, the sound of distant thunder terrified Broscovak so much that he refused to climb any farther and ended up on the floor of a cave in the fetal position, crying, for 45 minutes. “It was devastating,” he says. “It was a nervous breakdown. I thought I would never climb again.”
Now and then, Broscovak, 57, told doctors about the lightning strike and fugue states, but generally they didn’t take him seriously. “I would always bring up that I was struck by lightning, and they would just say, ‘Oh, that’s curious.’ ” Since the accident, Broscovak’s symptoms have become more manageable. A sinewy, bearded man with tan skin and green eyes, he has returned to climbing. Ironically, he has also continued his longtime career—as an electrician. (“In those situations,” he explains, “I control the electricity.”) Like Pandolph-Peary, he owes his recovery primarily to time, not medical interventions.
Time, meanwhile, hasn’t led to any significant research breakthroughs to explain his condition. After more than three decades of examining lightning victims, Mary Ann Cooper still can’t definitively  say what causes the chronic symptoms experienced by survivors like Broscovak, Pandolph-Peary, and Utley. But she has some ideas.
 Phil Brocovak struck lightning survivor
Lightning surivivor Phil Broscovak. (Ethan Hill / REDUX Pictures)
The evidence suggests that lightning injuries are, for the most part, injuries to the brain, the nervous system, and the muscles. Lightning can ravage or kill cells, but it can also leave a trail of much subtler damage. Cooper and other researchers have speculated that chronic issues are the result of lightning scrambling each individual survivor’s unique internal circuitry. She points out that even tiny amounts of electricity zipping through the body can permanently alter the behavior of neurons and other cells, which, in order to function correctly, depend on carefully orchestrated changes in the number of charged particles on either side of their membranes.
One of Cooper’s studies seems to support this theory. Using functional magnetic resonance imaging (fMRI), which traces blood flow in the brain, Cooper found statistically significant differences in brain activity between lightning-strike victims and healthy people as they performed mental-aptitude tests inside the scanner. Her results have been published, but she isn’t currently pursuing funding for further research. It’s just not important enough to most doctors and scientists, she says. These days she’s more focused on helping build awareness and preventing lightning injuries than looking into long-term symptoms.
Faced with a medical community largely unable to help them, survivors frequently turn to one another. The U.S. is home to at least two conferences of lightning-strike survivors each year—one in Pigeon Forge, Tennessee, and the other in Lynchburg, Virginia. In November 2010, I spent a couple of days at the Lynchburg conference. Each morning, attendees who had been injured by lightning or other electrical accidents gathered at a brick building owned by the American Legion. The first day began with the Pledge of Allegiance and silent prayer. Then, one by one, survivors stepped up to the podium and shared their stories.
A man in a bright orange shirt explained how he lost both his arms to an accident with an electrical cable. Others described the confusion they felt after recovery. Antoinette Palmisano spoke of the day in 1991 when she was struck by lightning inside a home office in Syracuse, New York. Lightning surged through the house’s electrical wiring, leaped out of a panel of switches like a poltergeist, seized Palmisano’s body, and threw her ten feet across the room. Today, Palmisano still suffers from acute fatigue and has trouble remembering simple information. She plasters her home with Post-it Notes and places timers in every room. The scribbled instructions and alarms remind her about appointments, errands, and daily tasks that most of us easily juggle in our heads.
One of the first people I met in Lynchburg was a woman in a wheelchair who told me her name was Butterfly. She was wearing a loose purple shirt, khakis, weathered hiking boots, two pairs of sunglasses—one of which had cartoon eyes on the lenses—and a dream-catcher necklace. Butterfly claimed to have been struck by lightning on three separate occasions in the span of 41 years. Her body often quivered uncontrollably. She said she could no longer walk, that she had lost all ability to detect temperature, and that she felt like her bone marrow had evaporated, leaving her brittle. She stressed the importance of potassium supplements for survivors. She also admitted that the official diagnosis she received from doctors was conversion disorder, in which bodily symptoms are the manifestation of psychological stress rather than the result of physical damage. A few doctors had suspected her of malingering.
Listening to other, similarly curious accounts, it became clear that some lightning-strike survivors fabricate or exaggerate parts of their stories—whether intentionally or not. A few claimed to have suddenly developed bizarre powers after the strike. I have spoken with survivors who are adamant that they give off energy that somehow shortens the lives of electronic devices or makes streetlights go dark when they walk beneath them, that they can sense an approaching thunderstorm, or that lightning is more attracted to them than to people who have not been hit.
 Michael Utley lightning struckbylightning.org struck by survivor golf course coma
Lightning survivor Michael Utley. (Ethan Hill / REDUX Pictures)
To some survivors, these more outlandish claims only serve to reinforce the idea that their very real issues are suspect, too. “I have met people who say they have been struck three times and say the can see the future, play the piano, fuck all night long,” says Utley. “It’s all bullshit.”
About a year into his recovery, Utley attended his first survivors’ conference in Tennessee. There he met Dr. Cooper, who asked him to help launch the National Oceanic and Atmospheric Association’s first official lightning-safety awareness week, which kicked off in June 2001. Its maxim: “When thunder roars, go indoors.” The next year, Utley created Struckbylightning.org—a website devoted to educating people about preventing lightning injury—and started regularly speaking at schools and to Boy Scout troops and doing guest spots on televised weather reports. He continues to make educational outreach a priority, but he has not attended a survivors’ conference in quite some time.
“When I first got hurt, the conferences were the best thing in the world,” Utley says. “You’re out there saying, ‘I was struck by lightning,’ and most people don’t get it. These people understood. But as you get better, you tend not to go to them.”
Utley has trained his mind on the future. Despite the personality change and relentless pain—despite the hunger for an explanation that would make sense of it all—he no longer fixates on a “why” that probably doesn’t exist. You might wonder if you were chosen by that bolt, you might be suffering from mysterious symptoms, you might feel like an entirely different person, but it’s best not to ask why. “Yeah, I was pissed at first—I was pissed at the whole world,” Utley says. “I woke up and I couldn’t walk, couldn’t swallow, couldn’t do anything. What happened, and why? Why did I get struck and not the three guys 15 feet away from me? There’s no rhyme or reason. You can ask questions all you want, but it’s like yelling at the ocean. It does not answer back.” 

Fay-Wei Li stepped out of his car and looked around. There was not much to see aside from an old wooden fence and a soggy ditch strewn with roadside detritus. Could this really be the spot? A biologist at Duke University, Li had driven seven hours from North Carolina to these exact coordinates in Florida in search of hornworts: the living descendants of some of the very first land plants.

An hour’s search yielded nothing except an uneasy feeling about trespassing on nearby residences. Before giving up, Li checked one more spot. There, in that ditch full of trash, he found them. Most people would probably mistake the spring-green bristles for blades of grass, but Li recognised the hornworts right away. He plunged his hands into the soil, scooped up the rootless plants, and packed them in a plastic cooler. A humble package of earth and herbage, but one that would rewrite a chapter in the evolutionary history of plants. Long ago, hornworts did something plants are not supposed to do: they breached the species barrier, trading DNA with an entirely different kind of plant – a fern.

Between 300 and 130 million years ago, as trees and flowering plants grew to dominate the globe, the sun-loving ferns of yore found themselves trapped beneath forest canopies. Most fern species perished under this umbrage, but the ones that survived learned to live on lean light. These persistent plants evolved a molecule called neochrome that could detect both red and blue light, helping them stretch towards any beams that managed to filter through the dense awning of leaves.

Neochrome’s origins have long eluded scientists. As far as anyone knew, the gene that codes for neochrome existed in only two types of plants separated by hundreds of millions of years of evolution: ferns and algae. It was extremely unlikely that the gene had been passed down from a common ancestor, yet somehow skipped over every plant lineage between algae and ferns. About two years ago, while searching through a new massive database of sequenced plant genomes, Li found a near-exact match for the neochrome gene in a group of plants not previously known to possess the light-sensitive protein: hornworts. Through subsequent DNA analysis of living specimens – like those he collected in Florida – Li confirmed his suspicion: ferns did not evolve neochrome on their own; rather, they took the gene from hornworts.

The fern’s lifecycle offers some clues as to how this happened. Ferns alternate between two distinct modes: their familiar feathery adult forms, and glistening heart-shaped lobes known as gametophytes. Gametophytes produce and secrete sperm and eggs that must fertilise each other – or find gametes on another gametophyte – in order to produce a new adult fern. Thus exposed, the fern’s gametes could easily come into contact with the similarly liberated sperm and eggs of hornworts, which tend to congregate in the same moist spots on the forest floor. If damaged or malformed gametes from both plants found one another, they could have traded DNA across their broken membranes before fusing with one of their own kind.

Scientists have known for many decades that prokaryotes such as bacteria and other microorganisms – which lack a protective nucleus enveloping their DNA – swap genetic material with each other all the time. Researchers have also documented countless cases of viruses shuttling their genes into the genomes of animals, including our own.

What has become increasingly clear in the past 10 years is that this liberal genetic exchange is definitely not limited to the DNA of the microscopic world. It likewise happens to genes that belong to animals, fungi and plants, collectively known as eukaryotes because they boast nuclei in their cells. The ancient communion between ferns and hornworts is the latest in a series of newly discovered examples of horizontal gene transfer: when DNA passes from one organism to another generally unrelated one, rather than moving ‘vertically’ from parent to child. In fact, horizontal gene transfer has happened between all kinds of living things throughout the history of life on the planet – not just between species, but also between different kingdoms of life. Bacterial genes end up in plants; fungal genes wind up in animals; snake and frog genes find their way into cows and bats. It seems that the genome of just about every modern species is something of a mosaic constructed with genes borrowed from many different forms of life.

‘What scientists have seen is just a little tip of an immense iceberg,’ says Antonio Teixeira, a biologist at the University of Brasilia. W Ford Doolittle, a biochemist at Dalhousie University in Nova Scotia, agrees: horizontal gene transfer, he wrote recently ‘is far more pervasive and more radical in its consequences than we could have guessed just a decade ago’. Researchers have now discovered so many examples of gene transfer between species and kingdoms of life – with many more surely to come – that they have to adjust their understanding of how evolution works. Standard evolutionary theory does not account for the possibility of complex organisms suddenly acquiring genes from other species, let alone how those foreign genes might change a creature for better or worse. Think of it this way: if the genomes of living species are flowers on different branches of the great evolutionary tree of life, horizontal gene transfer is a subversive wind whipping pollen from one part of the tree to another.

Sign up for Aeon’s Newsletter

Email address
Daily
Weekly
SUBSCRIBE
The first hints of horizontal gene transfer among complex organisms emerged several decades ago. In the 1940s, at Cold Spring Harbor Laboratory in New York, Barbara McClintock discovered that certain genes in corn plants could pop out of one position on a chromosome and move to another. The extent to which this transposition happened in a particular kernel determined its unique pattern of colourful speckles. McClintock’s pioneering work demonstrated for the first time that a genome is highly dynamic, not forever fixed in one order.

That was a difficult concept for many scientists to accept. By the 1970s, however, other researchers had discovered ‘jumping genes’, or transposons, in much more than just corn, and the scientific community at large finally began to celebrate McClintock’s work, which earned her the Nobel Prize in 1983. Scientists now know that transposons are extremely abundant and often constitute large portions of a given genome: they make up more than 85 per cent of the maize genome and about half of our own. Some slice themselves out of one spot on a chromosome and move to another; others take a copy-and-paste approach, quickly multiplying. To make these jumps, transposons rely on two main strategies: either they include a genetic sequence encoding an enzyme known as transposase, which can chop a transposon out of its current location and reintroduce it elsewhere; or they use a different set of enzymes to produce strings of RNA that are translated into DNA and woven back into the host genome.

These are exactly the notions that have unravelled in the past decade as researchers have turned up one new case of gene transfer after another

Around the time of McClintock’s vindication, scientists stumbled upon a particularly prominent transposon in fruit flies. At the University of Arizona, Margaret Kidwell was mating laboratory-raised females of the fruit-fly species Drosophila melanogaster with males caught from the wild. Kidwell was surprised to discover that the offspring of her matchmaking were sterile and rife with crippling genetic mutations.

Further experiments revealed that the source of these aberrations was a transposon later dubbed the P element, and that this mobile gene had infiltrated just about every wild population of D melanogaster sometime in the previous 50 years. By confining some groups of fruit flies to laboratories for so many decades, scientists had protected them from this infestation. Whereas wild flies had evolved strategies to repress the genetic chaos triggered by the P element, laboratory strains had not. So their hybrid offspring were vulnerable. Making things even stranger, researchers discovered that the P element originally jumped to wild D melanogaster populations from another fruit fly species, Drosophila willistoni.

Although the two fly species live in the same areas, they are sexually incompatible – so how did the P element make its extraordinary leap? One of Kidwell’s colleagues, Marilyn Houck, suspected that a mite known as Proctolaelaps regalis was the gene-smuggler. The mite regularly parasitises both D melanogaster and D willistoni, using its needling mouthparts to suck up nutrients from fruit fly eggs and larvae. Such a parasite could conceivably transfer DNA from the egg of one fruit fly species to another. Follow-up studies showed that mites feeding on fruit flies did indeed harbor the P element.

The P element was a dramatic example of just how dynamic genes could be – of their potential to disregard the boundaries between different species’ DNA and shape an organism’s evolution. Horizontal gene transfer was partly responsible for reproductively isolating lab-bred populations of fruit flies from wild ones – a major step on the way towards speciation. Still, most biologists viewed horizontal gene transfer among insects and other animals as something of an anomaly. Yes, bacteria and viruses exchanged DNA on a daily basis. But when it came to animals, plants and fungi, such genetic trespassing was surely rare overall and, in most cases, of little importance.

These are exactly the notions that have unravelled in the past decade as researchers have turned up one new case of gene transfer after another. ‘There was a time when we didn’t even realise that transposons could come from other species,’ says Cedric Feschotte of the University of Utah. ‘Now it seems our own genome is a patchwork of raw genetic material coming from different places with different histories – that to me is very profound. Even the largest eukaryote genomes have this patchwork origin to them.’

In the mid-2000s, Feschotte and his colleagues noticed some unusual patterns among the sequenced genomes of various mammals. Again and again, the lineage of certain DNA segments failed to align with established evolutionary relationships. They would find, for example, nearly identical sequences of DNA in mice and rats, but not in squirrels; and the same sequence would turn up in nocturnal primates known as bushbabies, but not in other primate species. It was highly unlikely that mice, rats and bushbabies had independently evolved the exact same chunk of DNA. Further complicating things, these puckish strings of DNA were not in the same position on the same chromosome in different species, as you would expect if they had been inherited the traditional way – rather, their locations were highly variable.

On its epic journey through the tree of life, BovB has jumped between species at least nine times, and seems to have moved from reptiles to mammals

The reason, Feschotte and colleagues discovered in 2008, is that these DNA sequences were not vertically inherited genes; rather, they belonged to a widespread family of transposons, which the scientists dubbed SPACE INVADERS, or SPINs for short. SPINs have managed to insert themselves into the genomes of tenrecs, little brown bats, opossums, green anole lizards and African clawed frogs, in addition to bushbabies, mice and rats. In each of these species’ genomes, the transposons have multiplied either themselves or abbreviated forms of themselves thousands of times. And, in at least one case, mice and rats have adopted a SPIN transposon as one of their own, turning it into a functional gene that is actively read by the cellular machinery that translates genes into proteins, though its exact role remains a mystery. Over the past 30 million years, several SPINs have infiltrated the little brown bat’s genome and replicated an enormous number of times. This amplification coincides with one of the swiftest periods of speciation in the bat’s evolutionary history. It is by no mean’s conclusive proof that horizontal gene transfer encouraged the speciation, but it is indicative.

A different kind of transposon – one of the copy-and-paste variety – has spread through an equally diverse group of animals. In 2012, David Adelson, Ali Walsh at the University of Adelaide, and their colleagues, discovered that the transposon BovB – first found in cows (hence the bovine epithet) – is also present in anoles, opossums, platypuses, wallabies, horses, sea urchins, silkworms and zebrafish, to name a few. Once again, vertical inheritance via traditional evolutionary relationships could not explain the transposon’s haphazard materialisation here and there. On its epic journey through the tree of life, BovB has jumped between species at least nine times, and seems to have generally moved from reptiles to mammals.

How does one little piece of DNA get into all those distantly related creatures living in such different places – animals that likely never even encountered one another, let alone mated? It probably enlists the help of organisms that have mastered the art of hitchhiking: ticks. Adelson, Walsh and colleagues found BovB in several tick species known to vampirise reptiles. Likewise, a couple of years after first discovering SPINs, Feschotte and colleagues found them yet again in two creatures that – just like the mite with an appetite for fruit fly eggs – have the potential to transmit transposons from one animal to another: a blood-sucking insect known as the kissing bug (Rhodnius prolixus), which feeds on birds, mammals and reptiles alike; and the pond snail (Lymnaea stagnalis), which is host to many parasitic flatworms that infect various vertebrates. Alone, the kissing bug and pond snail cannot explain all of SPINs’ conquests; their habitats overlap with many but not all of the vertebrates that contain the transposons. But the available evidence suggests that this six-legged parasite and shelled parasite hotel are two key accomplices that allowed SPINs to infiltrate so many different animal lineages within the past 50 million years.

Sometimes, parasites transfer far more than a single gene into the genomes of their hosts. Like many insects, the fruit fly species Drosophila ananassae is home to parasitic bacteria known as wolbachia, typically found in an insect’s sex organs. Through a series of gene‑sequencing studies, scientists have confirmed that the wolbachia species living inside D ananassae has shuttled not just one, but all of its 1,206 genes into the fruit fly’s DNA. Consider this: insects are collectively the most numerous animals on the planet; wolbachia infects between 25 and 70 per cent of all insect species, and it’s probable that wolbachia has successfully completed such genetic mergers in far more than fruit flies. Think of the quintillions of insects in the world – all those buzzing, bristling, bug-eyed creatures. At their very core, most of them might not be individual organisms but at least two beasts in one.

Recently, while studying a virus that preys on wolbachia, Jason Metcalf and Seth Bordenstein of Vanderbilt University in Tennessee discovered the Napoleon of horizontal gene transfers: a little gene that has conquered every kingdom of life. The virus in question attacks and kills wolbachia using a gene named GH25-muramidase, which encodes an enzyme that can perforate bacterial cell walls. When Metcalf and Bordenstein traced the evolutionary lineage of GH25, they discovered a pattern of inheritance that looked anything but typical. The GH25 gene was scattered throughout the tree of life: in bacteria, plants, fungi and insects. This particular gene seems to have moved fluidly through the microbial world and then hopped laterally to viruses, plants, fungi and insects living in close association with different kinds of bacteria. ‘Every organism needs to fight bacteria off,’ Metcalf says. ‘If they can get a new method of antibacterial defence, that’s a huge evolutionary advantage for them.’

In Japan, some people’s gut bacteria have stolen seaweed-digesting genes from ocean bacteria lingering on raw seaweed salads

One of the most clear-cut instances of horizontal gene transfer is the story of the fungus and the pea aphid. Some fungi, plants and bacteria have genes encoding carotenoids, a diverse class of colourful molecules involved in everything from photosynthesis and vision to camouflage and sexual attraction. No one had ever found such genes in animals, though. In all known cases, animals acquired carotenoids from their diet (for instance, flamingoes become red and pink from eating plankton). In late 2009, Nancy Moran, an evolutionary biologist then at the University of Arizona, stumbled onto the fact that pea aphids have a carotenoid gene.

‘More than 270 million years ago, a lone aphid likely attained a carotenoid gene from a fungus’. Photo courtesy Wikipedia
Scientists already knew that pea aphids appear green or red depending on the carotenoids in their bodies, and that aphid populations shift their colours in response to certain threats: green aphids are more susceptible to parasitic wasps; red aphids are more vulnerable to ladybirds. But the origin of the pigments had always been something of a mystery. Aphids primarily feast on sap, which does not contain many carotenoids. And pea aphids were often found with very different carotenoids than the ones inside the plants they were eating. When Moran compared the aphid’s pigment genes with those in many different creatures, the closest match was in a family of fungus. More than 270 million years ago, a lone aphid likely attained a carotenoid gene from a fungus – perhaps one that was infecting it, or one it was munching. Other scientists have since discovered that spider mites and gall midges have also acquired carotenoid genes from fungi and bacteria.

Shake any branch on the tree of life and another astonishing case of interspecies gene transfer will fall at your feet. Bdelloid rotifers – tiny translucent animals that look something like sea slugs – have constructed a whopping eight per cent of their genome using genes from bacteria, fungi and plants. Fish living in icy seawater have traded genes coding for antifreeze proteins. Gargantuan-blossomed rafflesia have exchanged genes with the plants they parasitise. And in Japan, some people’s gut bacteria have stolen seaweed-digesting genes from ocean bacteria lingering on raw seaweed salads.

At this point, the tally is too high to ignore. Scientists can no longer write off gene-swapping among eukaryotes – and between prokaryotes and eukaryotes – as inconsequential. Clearly genes have all kinds of ways of journeying between the kingdoms of life: sometimes in large and sudden leaps; other times in incremental steps over millennia. Granted, many of these voyages are probably futile: a translocated gene finds itself to be utterly useless in its new home, or becomes such a nuisance to its genetic neighbours that it is evicted. Laterally transferred genes can be imps of chaos, gumming up or refashioning a genome in a way that is ultimately disastrous – perhaps even lethal to a species. In a surprising number of instances, however, wayfaring genes make a new life for themselves, becoming successful enough to change the way an organism behaves and steer its evolution.

The fact that horizontal gene transfer happens among eukaryotes does not require a complete overhaul of standard evolutionary theory, but it does compel us to make some important adjustments. According to textbook theories of evolution, the major route of genes moving between organisms is parent to child – whether through sex or asexual cloning – not this sneaky business of escorting genes between unrelated organisms. We must now acknowledge that, even among the most complex organisms, vertical is not the only direction in which genes travel.

Likewise, standard theory says that mutations are supposed to happen within a species’s own genome, not come from somewhere else entirely. We now know that the appearance of new genes does not necessarily result from tweaks to native DNA, but might instead represent the arrival of far-flung visitors. ‘We need to start thinking about genomes as ecological units rather than monolithic units,’ says Jack Werren of the University of Rochester in New York, one of the scientists who discovered the wolbachia/fruit fly Russian doll. ‘We’re dealing with a new category by which unique genes can evolve.’

In some cases, this genetic hopscotching ‘could exert a very powerful evolutionary force’, says Li. ‘It can introduce novelties that cannot be achieved by gradual genetic mutations.’ Consider that a plant acquiring a gene from a bacterium, or an aphid from a fungus, is not receiving some half-constructed genetic prototype. Rather, it gets the benefit of all the aeons of natural selection that have whittled that gene in another creature, honing its power. An introduced gene might need some tweaks before it whirs in sync with its new neighbours, but it could be closer to such harmony than a de novo mutation that was caused by, say, a cell-division error or UV radiation. Horizontal gene transfer opens the possibility of a creature instantaneously acquiring a gene-trait combo that its own genome would have been unlikely to invent by itself.

Laterally transferred genes can sway evolution’s tiller in more subtle ways, too. Certain types of introduced genes duplicate themselves many times over, often leaving behind either little bits and pieces or entire replicas. In the process, they can rearrange large chunks of native DNA, change the way certain genes are expressed, or create whole new genes out of all this shuffling. By making a host genome larger and more diverse, these genetic immigrants increase the probability of copying and editing errors, some of which can be serendipitous and spur rapid evolution, as might have happened with the little brown bat.

We can unite these various corollaries to standard evolutionary dogma by re-imagining the tree of life. In the classic textbook depiction, the tree of life has a single trunk that splits into three big domains – bacteria, archaea (which resemble bacteria but are genetically and molecularly distinct) and eukaryota. These three domains of life branch into all known species. Every creature that ever existed presumably ‘descended from some one primordial form’, as Charles Darwin put it in 1859. And genes ostensibly flow in one direction: up from the trunk.

Scientists such as Ford Doolittle and Carl Woese at the University of Illinois have argued that this portrayal is an oversimplification. Rather than rising from a single trunk, they say, the tree of life stands on an interweaving root system. Rather than evolving from one ‘last universal ancestor’, all life arose from a communal pool of primitive cells with unbridled zeal for exchanging DNA. For relatively simple cells with only a handful of genes each, swapping DNA was an excellent strategy for acquiring and preserving the best adaptations around.

At some point, Woese proposed, cells reached a certain threshold of complexity at which it became detrimental to embrace a bombardment of foreign genes. A primordial cell harbouring a small group of genes can potentially gain a lot by adding new genes to its repertoire; but a more sophisticated cell with hundreds or thousands of genes risks imbalancing an intricate genome fine-tuned by a longer period of natural selection. So, complex eukaryotic cells evolved new ways to protect their DNA and expunge genetic invaders.

However, as has become clear in the past decade, horizontal gene transfer did not halt among eukaryotes and their microbial denizens. A mischievous breeze continued to blow DNA this way and that, from one branch on the tree of life to another. Wolbachia, pea aphids and hornworts all encourage us to accept a truth that seems unsettling at first, but ultimately invites us into greater communion with all life on the planet.

We can no longer pretend that gene-mixing between species is ‘unnatural’, that it is some misguided practice that would never exist if not for our meddling latex-gloved hands

There seems to be a notion in the public consciousness that the DNA of one species should not mix with the DNA of another. This belief becomes especially clear in the ongoing debate about genetically modified organisms (GMOs). Opponents frequently argue that the kind of gene transfers scientists make between different species would never happen outside the lab. Putting a wheat gene into a chestnut tree, or a bacterial gene into corn, or a fish gene into a tomato? Surely that’s unnatural. The ostensible perversion of mixing genes is struck like a gong, again and again. The supermarket chain Whole Foods, for example – which counsels its customers on how to avoid genetically modified foods – defines GMOs as ‘organisms whose genetic make-up (DNA) has been altered in a way that does not occur naturally.’

But it does. Genetic promiscuity is far more prevalent in nature than we realised. This fact alone is not an argument in favour of GMOs; simply because something occurs in nature without assistance from humans does not mean it is inherently good or bad. Confronted with this fact, however, we can no longer pretend that gene-mixing between species is ‘unnatural’, that it is some misguided practice that would never exist if not for our meddling latex-gloved hands. We did not invent gene transfer; DNA did. Genes are concerned with one thing above all else: self-perpetuation. If such preservation requires a particular gene to adapt to a genome it has never encountered before – if riding a parasite from one species to another turns out to be an extremely successful way of guaranteeing perpetuity – so be it. Species barriers might protect the integrity of a genome as a whole, but when an individual gene has a chance to advance itself by breaching those boundaries, it will not hesitate.

That’s the thing about DNA: its true loyalty is to itself. We tend to think of any one species’s genome as belonging to that species. We have a strong sense of ownership over our genes in particular – an understanding that, even though our genome overlaps with that of other creatures, it is still singular, is still ‘the human genome’. So strong is our possessiveness that the mere idea of mixing our DNA with another creature’s – of any two species intermingling genes – immediately repulses us. As far as DNA is concerned, however, the supposed walls between species are not nearly so impermeable. Up in the branches of the great tree of life, we are no longer immersed in the ancient communal pool that watered its tangled roots. Yet we cannot escape the winds of promiscuity. Even today – as was true from the start – ‘our’ genes are not ours alone.

the morning of July 13, like most mornings, Stephen Jones’s laboratory in Mount Vernon, Wash., was suffused with the thick warm smell of baking bread. Jones walked me around the floor, explaining the layout. A long counter split the space down the middle. To the right was what Jones called ‘‘the science part,’’ a cluster of high-tech equipment designed to evaluate grain, flour and dough. Jones, who is 58 and stands a daunting 6 foot 5, calls to mind a lovably geeky high-school teacher. He wore dungarees, a plaid shirt, a baseball cap and a warm, slightly goofy smile. Two pairs of eyeglasses dangling from his neck jostled gently as he gesticulated, describing the esoteric gadgetry surrounding us. The 600-square-foot room, known as the Bread Lab, serves as a headquarters for Jones’s project to reinvent the most important food in history.

Jones pointed to a sleek red machine, roughly the length of three toasters. ‘‘This one’s an alveo­graph,’’ he said, smirking. ‘‘It blows bubbles.’’ If a globe of dough inflates to the size of a baseball without bursting, that means it has enough elasticity and extensibility to make a baguette or a rustic loaf. ‘‘But if it just goes fffft, it’s probably going to be at best a scone or cookie,’’ Jones said. Nearby was a squat device that looked like a photocopier — a farinograph, which assesses the strength of dough as it is mixed — and a cylindrical machine that tests raw grain for adequate levels of starch.

‘‘You put all three of those together, and you get a very good idea of what type of product that’s going to bake,’’ Jones said. ‘‘Then you come over here’’ — we moved to the left side of the room — ‘‘and you have everything that a craft baker would be familiar with.’’ There was a wooden baker’s bench, wicker nests for rising dough, a steam-injected hearth oven full of crispening boules, an assortment of hand-operated mills. And there was flour: flour piled in bowls, flour coating every available surface, flour kicked up into the air as we walked by.

What most people picture when they think of flour — that anonymous chalk-white powder from the supermarket — is anathema to Jones. Before the advent of industrial agriculture, Americans enjoyed a wide range of regional flours milled from equally diverse wheats, which in turn could be used to make breads that were astonish­ingly flavorful and nutritious. For nearly a century, however, America has grown wheat tailored to an industrial system designed to produce nutrient-poor flour and insipid, spongy breads soaked in preservatives. For the sake of profit and expediency, we forfeited pleasure and health. The Bread Lab’s mission is to make regional grain farming viable once more, by creating entirely new kinds of wheat that unite the taste and wholesomeness of their ancestors with the robustness of their modern counterparts.

Although regional grain economies have developed in California, North Carolina, Arizona and elsewhere, there are few people who match Jones’s fervor for wheat and none with an equally grand vision for its future. His lab was founded just three years ago, but it has already earned the respect of the country’s most celebrated bakers, like Chad Robertson of Tartine and Jeffrey Hamelman, the director of King Arthur Bakery. Dan Barber teamed up with Jones to develop ‘‘Barber wheat’’ for his restaurant Blue Hill at Stone Barns, which is ensconced in a working farm. Bread Lab breads have even made their way to the kitchens of the White House.

Continue reading the main story
RELATED COVERAGE


THE FOOD ISSUE
The Archive of Eating OCT. 29, 2015

THE FOOD ISSUE
Edna Lewis and the Black Roots of American Cooking OCT. 28, 2015

THE FOOD ISSUE
Betty Crocker’s Absurd, Gorgeous Atomic-Age Creations OCT. 27, 2015
RECENT COMMENTS

SE November 2, 2015
If you are ever lucky enough to be on the south end of Whidbey Island in Puget Sound, you will have a chance to try some extraordinary...
Wheatgrower November 2, 2015
I hope this article was on the op-ed page as that is what it is. To label wheat as broken etc is just false. It is a matter of taste and...
NYHuguenot November 1, 2015
We still have about a dozen grain mills here in North Carolina. Smucker's discontinuation of Red Band caused a panic here as so many bakers...
SEE ALL COMMENTS
Advertisement

Continue reading the main story
In recent months, the lab’s newfound popularity has caused a bit of an identity crisis. Its latest collaborator is the fast-casual Mexican chain Chipotle, which wants to use one of the lab’s regional wheats in its tortillas. Chipotle serves 800,000 tortillas around the country every day. ‘‘There are definitely issues of scale,’’ Jones says. ‘‘If you have Chipotle come in, how big does it get, and how quickly? Do we end up with a commodity by any other name?’’

Newsletter Sign UpContinue reading the main story
Sign Up for the Magazine Newsletter
Get the best of the Magazine delivered to your inbox every week, including exclusive feature stories, photography, columns and more.


Enter your email address
 Sign Up

You agree to receive occasional updates and special offers for The New York Times's products and services.

SEE SAMPLE PRIVACY POLICY OPT OUT OR CONTACT US ANYTIME
Jones and wheat first met when he was a child. While learning to make bagels and marbled rye from his grandmother, Jones listened to tales of the wheat farms that her family had worked on in Poland. While studying agronomy at Chico State University in the late 1970s, Jones grew a modest five acres of wheat on a campus farm. ‘‘I fell in love with it as a crop,’’ he says. He would gaze upon his wheat every day, especially before sunrise and after sunset. ‘‘I don’t know if ‘spiritual’ is the right word, but it was very moving,’’ Jones says. ‘‘I would hear voices.’’ Around that time, he saw Terrence Malick’s 1978 film ‘‘Days of Heaven,’’ which is saturated with unhurried, sunset-lit shots of oceanic wheat fields in the Texas panhandle. ‘‘That did it for me,’’ he says.

A few years after college, Jones apprenticed with an Idaho wheat breeder named D.W. Sunderman, who taught him the craft of breeding: selectively cross-pollinating plants in order to create entirely new varieties. A head of wheat contains up to a hundred hermaphroditic flowers that usually pollinate themselves. Jones would choose a head on one wheat plant and pluck out all its pollen-producing anthers with tweezers, preventing self-pollination. Then, using plastic tubing or gauze, he would bind the neutered head to an intact one on a second wheat plant. Because wheat produces so many flowers, and has a gargantuan genome many times larger than our own, a single cross can yield a carnival of wildly different offspring. ‘‘He taught me how beautiful plant breeding could be,’’ Jones says of Sunderman, ‘‘and also the notion that if I wanted his job, I would have to get a Ph.D.’’

In 1991, Jones completed his doctorate in genetics at the University of California, Davis, and the U.S.D.A. hired him to study the wheat genome at Washington State University’s main campus in Pullman. Three years later, Jones landed a job as one of W.S.U.’s chief wheat breeders. At first, he was ecstatic, but disillusionment soon followed. The essence of plant breeding is innovation — the prospect of creating something truly novel. Yet in his first official role as a wheat breeder, Jones felt stifled. He was tasked with improving the yield and disease-resistance of wheat cultivars that had been designed for industrial milling. Prioritizing qualities the food industry considered superfluous was discouraged. When he tried breeding wheat with higher levels of nourishing minerals, like iron, zinc and magnesium, he was told those characteristics were unimportant. When he proposed working with a healthier wheat that still made excellent bread flour — albeit of a somewhat yellow tint — the university expressed no interest, he says.

Commodity wheats are defined in just three ways: hard (high in protein, which is good for bread) or soft (better for pastries); red (dark color and strong flavor) or white (pale and more delicate-tasting); and winter or spring, depending on when they are planted. ‘‘Hard red spring,’’ for example, is often used for bread; ‘‘soft white winter’’ is better for pastries. A vast majority of America’s 56 million acres of wheat grow in a belt stretching more than 1,000 miles from the Canadian border to Central Texas. Around half of the crop is exported, and most of what remains is funneled to feedlots for cattle or to giant mills and bread factories, which churn out all those bags of generic white flour and limp sandwich bread sleeved twice in plastic. This industrial system forces plant breeders to prioritize wheat kernels of highly specific sizes, colors and hardness.

By 2007, Jones had spent more than a decade begrudgingly breeding wheats for the commodities market. His opinion of industrial agriculture was no secret, however. As tensions mounted between Jones and the university, he made a bold decision: In order to escape the commodities system, he would give up wheat altogether. In 2008, he moved to W.S.U.’s western campus to become director of the W.S.U.-Mount Vernon Research Center, which helps small- and mid-scale farmers in the surrounding Skagit Valley, halfway between Seattle and Vancouver, grow about 80 different kinds of fruits, vegetables and flowers.

While preparing for the move, Jones thought he would end up working on cabbage for sauerkraut or cucumbers for pickling; he didn’t have a spiritual connection with those crops, but he liked them well enough. Driving around the area, how­ever, he was startled to discover one wheat field after another. Farmers told him it was crucial for crop rotations, which disrupt disease cycles and return nutrients to the soil. They harvested and sold the grain, but only to lose less money. There was no sense in trying to compete with giant growers in the nation’s wheat belt. What would happen, Jones wondered, if he developed unique varieties of wheat adapted to the Skagit’s cool, wet climate and extremely fertile soil? What if he could interest local millers and bakers in dealing primarily with Washington wheat? What if wheat, like wine, had terroir? After all, it used to.

Photo

Stephen Jones, director of the Bread Lab, at the Washington State University campus in Mount Vernon, Wash. Credit Ian C. Bates for The New York Times
The giant band of wheat that stripes the center of America is a byproduct of the industrial age. From the 18th century to the early 19th century, wheat was grown mainly near the coasts. During this time, immigrants and American emissaries introduced numerous varieties — Mediterranean, Purple Straw, Java, China, Pacific Bluestem — which breeders tinkered with, adapting them to various soils. All that preindustrial wheat was a living library of flavors: vanilla, honeysuckle, black pepper. Agricultural journals of the time noted the idiosyncrasies of wheat kernels — whether they were red and bearded, velvety or ‘‘plump, round, of a coffeelike form’’ — and distinguished wheats that produced ‘‘excellent’’ and ‘‘well-flavored’’ bread from those that yielded ‘‘inferior’’ loaves. Two wheats in particular, Red Fife and Turkey Red, became immensely popular in part because of their robust nuttiness.

As wheat spread from the coasts inward, so did flour mills. By 1840, 23,000 of them were scattered throughout the country. (Today there are around 200.) People would bring wheat they farmed themselves, or bought from nearby farmers, to the closest mill. As the culinary historian Karen Hess noted: ‘‘Even if you yourself did not grow wheat, you knew that what you bought was local wheat, and you knew its baking characteristics.’’ Bakers and homemakers were used to adjusting their recipes to accommodate different varieties. And they knew how to spot high-quality flour: ‘‘Good flour adheres slightly to the hand, and if pressed in the hand retains its shape,’’ advised ‘‘The Young Housekeeper’s Friend,’’ published in 1845.

In the mid-1800s, America’s patchwork of regional grain economies began to give way to a much more integrated system. The Erie Canal and transcontinental railroads opened up the vast expanse of the Great Plains to wheat farmers, while also offering affordable ways to ship grain across the country. By the late 1800s, wheat farming had largely shifted to Midwestern states.

At the same time, the Industrial Revolution fundamentally altered the process of turning grain into flour. Jones has a pantomime he likes to perform when discussing the millenniums-long history of milling: ‘‘First, there was pounding with mortar and pestle,’’ he says, smashing a fist into an open palm. ‘‘Then, there was this’’ — he moves his hands back and forth parallel to the ground, mimicking the manual grinding of grain between two slabs of stone. ‘‘Then, it turned into this,’’ he continues, rotating his hands around an invisible vertical axis to conjure the motion of the stone mill, which could be powered by water, wind or animal. ‘‘And then this.’’ He points his index fingers toward each other and spins them in sync.

That last transition, the steel roller mill — which first appeared in Budapest in 1839 — was a radical departure from previous techniques, because it sheared the wheat kernel apart. A grain of wheat has three main components: a fibrous and nutrient-rich outer coating called the bran; the flavorful and aromatic germ, a living embryo that eventually develops into the adult plant; and a pouch of starch known as the endosperm, which makes up the bulk of the grain. Before roller mills, all three parts were mashed together when processed. As a result, flour was not the inert white powder most of us are familiar with today; it was pungent, golden and speckled, because of fragrant oils released from the living germ and bits of hardy bran. If freshly ground flour was not used within a few weeks, however, the oils turned it rancid.

Roller mills solved this problem. Their immense spinning cylinders denuded the endosperm and discarded the germ and bran, producing virtually unspoilable alabaster flour composed entirely of endosperm. It was a boon for the growing flour industry: Mills could now source wheat from all over, blend it to achieve consistency and transport it across the nation without worrying about shelf life. That newfound durability came at a huge cost, however, sacrificing much of the grain’s flavor and nutrition. In the 1940s, to compensate for these nutritional deficiencies, flour producers started fortifying white flour with iron and B vitamins, a ubiquitous practice today. The rise of roller milling and bread factories also put pressure on plant breeders to make wheat even more amenable to the new dominant technologies; whiteness, hardness and uniformity took precedence over flavor, nutrition and novelty.

Photo

Loaves of 100-percent whole-wheat bread resting in the Bread Lab. Credit Ian C. Bates for The New York Times
Today, whole-wheat flour accounts for only 6 percent of all flour produced in the United States. And most whole-wheat products sold in supermarkets are made from roller-milled flour with the germ and bran added back in. According to the F.D.A.’s standards, flour labeled ‘‘whole wheat’’ must retain the germ, bran and endosperm in their native proportions, and ‘‘whole-wheat bread’’ must be made with whole-wheat flour, but the agency does not verify products’ composition before they hit the market.

It’s also unclear how industrial mills add back the oily germ without significantly shortening shelf life, a topic they are hesitant to discuss. David Killilea, a nutrition scientist at the Children’s Hospital Oakland Research Institute in California, says big mills might deactivate the living germ by steaming it or exposing it to gamma rays. In Michael Pollan’s most recent book, ‘‘Cooked,’’ one former General Mills employee confesses that the germ was thrown out because it was too much trouble. Killilea and his colleagues are currently developing a test to determine the proportions of endosperm, germ and bran in an anonymous sample of flour, which could bring transparency to the murky practices of industrial mills.

When Jones arrived at Mount Vernon, he started digging through seed banks, collecting both heirloom wheats and modern varieties, crossbreeding them and adapting them to the Skagit Valley’s climate. In 2010, he had some of his favorite varieties stone-milled and then brought several five-pound bags of flour to the Seattle-based baker George De Pasquale for an expert evaluation.

De Pasquale took handfuls of flour and crushed them in his fist to assess their moisture. He quickly mixed the flours with water and yeast and scooped up a portion to taste. Finally, he baked a baguette, a batard and a miche with each flour. ‘‘A couple did not have much flavor or structure,’’ he says, ‘‘but one of them in particular, Bauermeister, knocked my socks off.’’ The bread came out of the oven dark brown. He sliced into it, squeezed the loaf near his face and breathed in: There was a noticeable aroma of chocolate, an alcoholic twang and hints of cinnamon and nutmeg. He took a bite. ‘‘The flavors were so deep, so complex,’’ De Pasquale says. ‘‘The cell structure was really beautiful too. Giant holes, chewable and soft. I’d never been whacked on the head like that before.’’

Inspired by De Pasquale’s reaction, Jones ramped up his efforts to breed for ‘‘flavor, nutrition, funkiness.’’ Each year, Jones and his senior scientific assistant, Steve Lyon, along with three graduate students — Bethany Econopouly, Brigid Meints and Colin Curwen-McAdams — grow between 5,000 and 10,000 kinds of wheat. So far the Mount Vernon breeders have produced wheat with higher than typical levels of iron and other micronutrients; grains that are strikingly blue, purple and black; and wheats that imbue bread with maltiness, spice, caramel — a whole palette of flavors most people would never expect. ‘‘Much as grapes acquire a sense of place, we are finding so does wheat,’’ Jones says. This fall, he plans to publicly release two lines: a West Coast-adapted version of a French wheat named Renan, and a hard red winter wheat called Skagit 09, each of which makes excellent, nutty, moderately dark bread. Once released, the cultivars will either be freely distributed or sold affordably to farmers.

At first, it was just Jones baking bread from his wheat. Two years ago, having reached his ‘‘limit of credibility,’’ he decided to hire a resident baker. He settled on Jonathan Bethony, a slender man who sports a thick chestnut beard and has studied with some of the best in his field. With Bethony onboard, the Bread Lab’s reputation grew even faster, drawing in an ever-widening circle of bread masters and renowned chefs. The lab also became the host of the Grain Gathering, an annual conference that attracts 250 grain obsessives from all over the country. Jones and his colleagues have now outgrown their cozy headquarters; by spring of next year, they will move to a 12,000-square-foot building that they are renting from the Port of Skagit. Their new home will feature a much larger lab and an educational center where King Arthur Flour will train 2,000 bakers each year.

Slide Show
The Other Frankenfoods, by Marnie Hanel

Go to previous slideGo to next slide Olympia Oyster: Overharvesting and water pollution depleted the Pacific Northwest’s only indigenous oyster to 5 percent of its historic stock. Now the Puget Sound Restoration Fund is helping to revive the Olympia’s slightly metallic sweetness. Credit Can Stock Photo Inc. 1 of 7 Go to previous slide Go to next slide
Encouraged by the Bread Lab’s success, a group of investors is planning to build a mill nearby, which will have the capacity to produce millions of pounds of flour annually. In part, this is an attempt to solve a huge challenge for large bakeries interested in freshly milled whole-wheat flour: inconsistency. Although the new mill may produce some single-variety stone-ground flours, it will focus on roller-milling wheat and blending different types to achieve uniformity. These first signs of concession to industrial techniques have alarmed some in Jones’s circle. Perhaps, they worry, the Bread Lab and its affiliated projects are getting too big and too popular for their own good. Is it really possible to scale up without becoming the very type of system Jones and the others fled?

Such concerns have been worsened by the Bread Lab’s new high-profile partner, Chipotle. Steve Ells, the company’s founder, learned of Jones around three years ago through conversations with Glenn Roberts of Anson Mills and Dan Barber. At the time, Ells was determined to meet his promise of ‘‘food with integrity,’’ of working with sustainably grown, locally sourced whole ingredients when possible. ‘‘The tortilla was the outlier,’’ he says: Chipotle’s was made with commodity flour, dough conditioners and chemical preservatives. A single trip to the Bread Lab convinced Ells that Jones and his team could replace Chipotle’s tortillas with a more healthful and tastier whole-wheat version.

After a few false starts, Bethony devised a viable recipe using just five ingredients: water, oil, salt, whole-wheat Edison flour and a sourdough starter. The starter, a living concoction of flour, water and microbes, is not exactly standard procedure for a tortilla, but it is crucial for Bethony’s version: It magnifies flavor, makes the wheat molecules more digestible and extends the tortilla’s shelf life by adding acidity. Bethony taught his techniques to Ruben Berber and Tom Hoffert, who work in research and development at Don Pancho Authentic Mexican Foods in Salem, Ore., Chipotle’s primary tortilla supplier in the Northwest.

To accommodate Ells’s vision, Don Pancho has seriously modified its production process. The company must continually prepare and store large volumes of sourdough starter, maintaining just the right temperature and pH level. Further complicating matters, whole-wheat flour is often temperamental. It absorbs much more water than most bakers expect, and the shards of bran threaten to tangle the smooth strands of gluten, creating a sandy texture. If something goes wrong, the tortillas might turn out too bitter, or not stretch to their full size, which would be disastrous for a burrito that typically envelops more than half a day’s recommended calories. So Don Pancho has been cutting Edison with white flour. The tortillas they have produced so far are dense, golden-tinged and slightly tangy.

Ells plans to test the semi-whole-wheat tor­tillas in the Pacific Northwest. If things go well, he says, he will pair regional sources of wheat with tortilla makers around the country, which means the tortillas in, say, California might taste different from those in Florida — a discrepancy that would make many chains cringe. Although Jones and his community celebrate Chipotle’s openness to such diversity, they have also been unnerved by its interest. When Jones started working with wheat farmers in the Skagit Valley, he hoped to ‘‘keep value where it was produced,’’ not to partner with a company that has more than 1,900 venues and serves 1.4 million customers each day. What if satisfying Chipotle deprives others?

Edison wheat grows primarily in one location: 500 acres of Oregon farmland managed by Jones’s collaborators Tom and Sue Hunton and their family. A few months ago, nearly all of that wheat had been reserved for Chipotle, which irked some of the Huntons’ customers, like Mel Darbyshire of Grand Central Bakery in Seattle and Nan Kohler, who owns Grist & Toll, the only stone mill in greater Los Angeles. ‘‘Businesses like mine and local bakers need to be baking with that Edison,’’ Kohler said at the time. ‘‘If Chipotle walks away, then what? We haven’t created something sustainable.’’

Since then, thanks to a bountiful harvest and a decreased quota from Chipotle, the Huntons have been able to sell Edison to Darbyshire and Kohler. Tom Hunton is still waiting on an official contract from Chipotle. Yet he is optimistic that the partnership will work out. He also sees this as an opportunity to extend the benefits of re­gional grain economies beyond their geographic borders. ‘‘In this case, even over distance, the connection between grower, baker and miller is much more cemented and transparent than other instances in a closer radius,’’ he says. ‘‘There’s a void out there that needs to be filled with something other than the industrial-mill model. This is one way to do it.’’

Despite the Bread Lab’s growing influence, it is still quite sealed off from most of American society. For those of us who are not ardent home bakers buying fresh whole-grain flour from one of the few recently resurrected stone mills, who do not regularly queue up at Tartine or dine at the likes of Blue Hill, the kinds of wheat and bread the lab extols are still largely inaccessible. That is the power of industrial agriculture: It has so thoroughly expunged genuine whole-wheat flour from our diet that most of us do not even notice its absence. And here is where the partnership with Chipotle just might make sense: If it succeeds, it will bring real whole wheat to more American plates than any other Bread Lab collaboration so far.

Of course, that all depends on how faithful Chipotle remains to Jones’s grand vision. If the final tortillas are less than 50 percent Edison, aren’t they just the restaurant equivalent of the imitation whole wheat from the grocery store? A month or so after touring the Bread Lab, I asked Jones one last time whether he was worried about compromising, now that his dream was giving way to an ambiguous reality. After all, this was a man who practiced daily communion with his wheat plants. His swift response surprised me: ‘‘Our job at the Bread Lab is not to get all religious on 100-percent whole wheat.’’ Then he caught himself. ‘‘Although,’’ he continued in a quieter voice, ‘‘I guess we do.’’

In the late 1990s, marine biologist Steven Haddock paid a visit to fellow scientist Osamu Shimomura at his laboratory in Woods Hole, Massachusetts. The two researchers shared an obsession with bioluminescence: light produced by chemical reactions in the bodies of living things—most famously the firefly, but also in fungi and a multitude of ocean creatures. At one point during their meeting, Haddock recalls, Shimomura poured what appeared to be large sesame seeds out of a jar and into his hand, dribbled some water onto them, and crushed them into a paste in his fist. Then he shut off the lights. His palm glowed a transfixing blue, as though it held a fairy.

The sesame seeds were in fact the dried bodies of tiny crustaceans known as ostracods. Shimomura explained that during the Second World War, the Japanese army harvested huge numbers of the creatures from the ocean. The cold blue light of umihotaru (sea fireflies) was bright enough for soldiers to read maps and correspondence, but too dim to give away their position to nearby enemies. “It was an easy, simple source of light,” says Shimomura, who is 87. “You just add water. Very convenient. You don’t need any batteries.” By the time Haddock visited Shimomura, the desiccated plankton were many decades old, yet they still retained their power to shine.

Haddock was so enchanted by this tale that he asked Shimomura if he could take a small portion of the ostracods back to his own laboratory at the Monterey Bay Aquarium Research Institute in California. He keeps them in a container no larger than a spice jar, which he rarely opens. “I’ve only tested it five or six times,” he says. But if you’re lucky, and the mood strikes, he just might take his little genie lamp off the shelf and conjure that ethereal glow.

 
The bioluminescence of ostracods, a type of crustacean, has played a unique role in maritime history, creating a light bright enough to light maps for soldiers to read. Photo by blickwinkel/Alamy Stock Photo
What is it about bioluminescence that we find so mesmerizing? Light, after all, is abundant. Each morning, an immense bowl of light lifts itself above the trees and rooftops, higher than birds and mountains, and spills its golden contents. Sunlight washes over the continents and oceans, dripping down forest canopies and pooling in valleys and deserts; it splashes silently across farms and cities; it slips into our bedrooms, seeps beneath our skin, and tunnels through our eyes to illuminate the theater of the mind. Yet we can’t seem to get enough light, or feel close enough to it. Throughout history, many cultures have told stories of people and beings wreathed with halos or imbued with an irrepressible brilliance: gods, angels, fairies, saints, and jinns. To be infused with light is to be divine or supernatural, precisely because it is an impossibility for us.

Failing to summon light from within, we found other ways to generate and control it, to keep it nearby even in the Sun’s absence: we tamed fire and channeled electricity; we learned to fling bombs of color against the veil of night and line our roofs with gleaming drops of rainbow; we devised powerful beacons that could be summoned at the flip of a switch and erected shining pillars along our streets. Today, some people are even willing to sew LEDs beneath their skin in order to backlight tattoos, or simply for sheer novelty. But it is all pretense. Despite our slick technology, we have never truly matched the ostracod or firefly. We cannot equal their intuitive mastery of illumination. Light is woven into their very biology in a way we have never known. “For an organism to make light, especially to have a big display of light, seems to us like a superpower,” Haddock says.

It’s a power we could not resist exploiting. For millennia, people have devised ingenious applications for bioluminescence, many of which are little known today. Roman naturalist and philosopher Pliny the Elder wrote that one could rub the slime of a certain luminous jellyfish, possibly Pelagia noctiluca, onto a walking stick to make it double as a torch. In the late 17th century, the physician Georg Eberhard Rumphius described indigenous peoples of Indonesia using bioluminescent fungi as flashlights in the forest. And before the 19th century, coal miners filled jars with fireflies, as well as dried fish skin crawling with bioluminescent bacteria, to serve as lanterns; the safety lamp had not yet been invented and carrying an open flame into a cave risked igniting explosive gas.

 
The bioluminescent jellyfish Pelagia noctiluca glows when it is disturbed. It also can create a luminous mucus. Photo by David Fleetham/Corbis
It took much longer for people to find uses for ostracods and other tiny gleaming sea creatures because, for most of human history, no one knew they existed. Early explorers puzzled over ribbons and specks of light around boats and oars, as well as radiant waves and regions of shining water sometimes known as “milky seas.” Initial attempts to explain such phenomena were often closer to poetry than science. For many, light was akin to fire, even if it was in water. Hai Nei Shih Chou Chi, a fourth- or fifth-century-BCE Chinese text detailing nautical adventures, states that, “one may see fiery sparks when the water is stirred.” Likewise, in the 17th century, French philosopher René Descartes likened the light seen in agitated seawater to sparks struck off flint. During a cruise to Siam in 1688, Jesuit missionary and mathematician Guy Tachard wrote that the Sun had ostensibly “impregnated and filled the sea during the day with an infinity of fiery and luminous spirits.”

In 1753, Benjamin Franklin surmised that some sort of “extremely small animalcule” in water “may yet give a visible light.” Around the same time, naturalists such as Godeheu de Riville, equipped with early microscopes, confirmed that Franklin’s hunch was correct: the ocean’s glints and glows emanated from living things, from tiny “marine insects” we now call plankton. By the early 20th century, bioluminescent plankton were far from unknown entities—they were under intense scrutiny by some of the world’s most powerful military forces, literally caught in the crossfire of human warfare.

When ships and other vessels pass through large groups of bioluminescent plankton, ripples and clouds of green and blue light often form at their sides and in their wake. Those unwanted spotlights have proven problematic for the navy, especially when stealth is required. In 1918, during the First World War, a British ship sunk a German U-boat off the coast of Spain after spying its glowing nimbus. By the Second World War and the Cold War, navies were studying how to track subs and torpedoes with bioluminescence. The United States Navy continues such research today, attempting to develop an aquatic robot that can measure bioluminescence as a way to both detect enemies and prevent an accidental reveal.

 
Bioluminescent organisms use their ability to glow for a variety of reasons, including defense. This cardinalfish spits out an ostracod, moments after trying to eat it. Photo by Nature Picture Library/Alamy Stock Photo
In 1954, ocean bioluminescence saved one military man’s life. At the time, future Apollo 13 astronaut James Lovell was a fighter pilot. He was on a training mission off the coast of Japan in stormy weather, when the instrument panel in his cockpit suddenly short-circuited. All the lights and dials went dark. He could no longer rely on technology to point him back to the aircraft carrier. Looking into the blackness, he noticed a shimmering green streak in the sea, which he realized was the ship’s bioluminescent wake. He used that trail of living light as a lodestar and managed to land safely.

The following year, Shimomura began his own affair with bioluminescence—one that would eventually revolutionize the field of biology. In 1955, Shimomura joined the lab of Yoshimasa Hirata at Nagoya University, where he was tasked with extracting the organic compound luciferin from ostracods and determining its precise molecular structure. Today, scientists know that in many bioluminescent organisms, an enzyme named luciferase catalyzes a chemical reaction between luciferin and oxygen, which produces light. But back then, “we didn’t fully understand how it happened,” Shimomura says. “It was a mystery.” After 10 months of labor in the lab, Shimomura became the first person to crystallize luciferin, an essential step in studying its structure.

In the 1960s, he continued his research at Princeton University, where he also started to investigate the luminous jellyfish Aequorea victoria. Shimomura and his colleagues collected numerous A. victoria specimens and strained them like cider apples to obtain a small amount of pure glowing “squeezate.” Within the shining liquid they discovered a protein they named aequorin, which releases blue light when it reacts with calcium, even in the absence of oxygen. Another protein in the jellyfish, green fluorescent protein (GFP), sometimes absorbs that blue light, and releases green light in response.

By 1978, after collecting nearly a million jellyfish, Shimomura had thoroughly elucidated the structure of aequorin and the nature of A. victoria’s unique light-generating reactions. Both aequorin and GFP—as well as the gene coding for the latter—became indispensable tools in biology and medicine. Scientists could now tag and observe the intricate dances of previously invisible genes and proteins in living cells. In 2008, along with Martin Chalfie of Columbia University and Roger Tsien of the University of California, San Diego, Shimomura received the Nobel Prize in Chemistry for his work on GFP.

 
Nobel prize-winning chemist and marine biologist Osamu Shimomura helped determine the nature of bioluminescence in the jellyfish Aequorea victoria. Photo by */Kyodo/Newscom
More recently, bioluminescence has evolved from laboratory tool to commercial plaything. The Kickstarter-funded, San Francisco-based Glowing Plant Project offers customers DIY kits they can use to genetically engineer a luminous Arabidopsis plant at home. And Carlsbad, California-based BioPop has released what is essentially an illuminated version of that long beloved novelty pet for kids, Sea-Monkeys (which are not in fact tiny aquatic primates, but rather brine shrimp). They call it Dino Pet: a small, vaguely Apatosaurus-shaped aquarium filled with bioluminescent plankton known as dinoflagellates. During the day, the plankton photosynthesize; at night, if you shut off the lights and give the aquarium a good shake, the dinoflagellates light up turquoise, much like the “fiery sparks” Chinese sailors observed in churning seawater so long ago. But the glow is only good for about three shakes a night, and if you’re too rough, you could damage or kill the plankton.

It’s easy to pity those tiny swimming stars trapped in a plastic bubble. Each night, some titan’s hand engulfs their ocean and churns it into a maelstrom for a few moments of selfish delight. Then the monster puts away their entire universe, easy as shutting the lid on a music box. They are kept alive solely for the purpose of this bedside magic trick.

Perhaps, though, we are the more pathetic members of this relationship—the gods bewitched by a gnat. Bottling bioluminescence gives us a sense of ownership over a presumably rare and otherworldly phenomenon; the reality of the situation is quite different. Bioluminescence is so commonplace on our planet—particularly in the oceans—that scientists estimate the thousands of glowing species they have catalogued so far are just a fraction of the sum. It may well be that the vast majority of deep-sea creatures, which live beyond the Sun’s reach, generate their own light (sometimes with the assistance of microbes). They use these innate glows primarily to communicate: to warn and frighten, hide and hunt, lure and beguile. Bioluminescence is one of the oldest and most prevalent languages on Earth—and one that is largely alien to us. Despite our fantasies and mythologies, the truth is that there’s nothing supernatural about living light; it has been a part of nature for eons. It’s just that we were denied this particular gift.

So, with perhaps too little gratitude, we adapted the incomparable talents of glowing creatures for our own purposes. We borrowed their light and it revealed things about our own biology we might never have discovered otherwise. But that is all we can do—borrow. We cannot be them, so we seek them out, and draw them near us—every bit as mesmerized as when we thought the Sun had impregnated the sea. To this day, we cup them in our hands, collect them in jars, and place them on our nightstand, forever trying to satisfy our Promethean hunger.

"The cat does not offer services," William Burroughs wrote. "The cat offers itself." But it does so with unapologetic ambivalence. Greet a cat enthusiastically and it might respond with nothing more than a few unhurried blinks. Later, as you're trying to work, it will commandeer your lap, keyboard, and attention, purring all the while. A cat will mew at the food bowl in the morning and set off on a multiple-day trek in the afternoon. Dogs are dependent on us to the point of being obsequious, but cats seem to be constantly reëvaluating the merits of our relationship, as well as their role in domestic life. "Are cats domesticated?" is one of the most frequently Googled questions about the animals, based on the search engine's autocomplete suggestions.
It's a question that scientists have been asking, too. The latest answer, based on insights from recent archeological discoveries and genome-sequencing studies, is that cats are semi-domesticated. Conventional wisdom holds that the ancient Egyptians were the first people to bond with the cat, only four thousand years ago. In 2004, however, a team of French researchers working in Cyprus unearthed the ninety-five-hundred-year-old remains of a human and a cat buried side by side. Last year, an analysis of cat bones and teeth from a fifty-three-hundred-year-old settlement in China indicated that the animals were eating rodents, grains, and the leftovers of human meals. It appears that, following the advent of agriculture, wildcats in the Near East and Asia likely began to congregate near farms and grain stores, where mice and rats were abundant. People tolerated the volunteer exterminators, and wildcats became increasingly comfortable with people. Whether this affiliation began five or ten millennia ago, the evidence suggests that cats have not been part of our domestic domain for nearly as long as dogs, which have been our companions for perhaps forty thousand years.
At first, the cat was yet another opportunistic creature that evolved to take advantage of civilization. It was essentially a larger version of the rodents it caught. Somewhere along the line, people shifted from tolerating cats to welcoming them, providing extra food and a warm place to sleep. Why? Perhaps because of the cat's innate predisposition to tameness and its inherent faunal charm—what the Japanese would call kawaii. Look up photos of the thirty-eight or so wildcat species and you might be surprised at how easy it is to picture one curled up on the couch. Dogs likely initiated their own domestication, too, by prowling around campfires in search of food scraps. Whereas our ancestors quickly harnessed dogs to useful tasks, breeding them to guard, hunt, and herd, they never asked much of cats. We have also been slow to diversify cat breeds. Many dog, horse, and cattle breeds are more than five hundred years old, but the first documented cat fanciers' show didn’t take place until 1871, at the Crystal Palace, in London, and the most modern cat breeds emerged only within the past fifty years.
This relatively short and lenient period of selective breeding is manifest in the cat genome, Wesley Warren, a geneticist at Washington University in St. Louis*, said. In a study published last year, Warren and his colleagues analyzed DNA from several wildcats and breeds of domestic cat, including an Abyssinian named Cinnamon. They confirmed that, genetically, cats have diverged much less from their wildcat ancestors than dogs have from wolves, and that the cat genome has much more modest signatures of artificial selection. Because cats also retain sharper hunting skills than dogs, abandoned felines are more likely to survive without any human help. In some countries, feral cats routinely breed with their wildcat cousins. "There's still a lot of genetic mixing," Warren said. "You don't have the true differentiation you see between wolf and dog. Using the dog as the best comparison, the modern cat is not what I would call fully domesticated."
Not all researchers agree. "I don't think it makes sense to talk about animals as semi- or fully domesticated," Greger Larson, a paleogeneticist and archeologist at Oxford University and an expert on domestication, said. "Any threshold you try to define will necessarily be arbitrary." Larson tends to agree with the views of Melinda Zeder, an archeologist at the Smithsonian Institution, who has written extensively on the domestication of both plants and animals. Zeder characterized domestication as an ongoing symbiosis between humans and another species—“a sort of pact that ends up being mutually beneficial,” she said. This relationship, she argued, can follow many paths and result in somewhat different outcomes, which she has catalogued. Sometimes people gradually domesticate a prey species—sheep, goats, cattle—or deliberately remove non-prey animals from the wild and breed them for a specific purpose, as we’ve done with horses. In other cases, hunger draws a wild animal—dogs, chickens, guinea pigs, cats—to human society, where it becomes increasingly tolerant of people. Even a single domestic lineage can contain varying degrees of dependency and a range of temperaments.
"Cats are domesticated," Zeder said. "But I think what confuses people about cats is that they still carry some of the more aloof behaviors of their solitary wild progenitors. Sometimes they don't give a damn about you, but they are very much part of your niche. Cats have us do everything for them. We clean their litter, stroke them, admire them, but unlike dogs they do not have to constantly please and satisfy our needs. They are probably the ultimate domesticate."
When I was growing up in California, I had a tuxedo cat named Jasmine. When I called her, she would sometimes stop in place and stare at me for a few minutes before trotting over, as though she needed to preserve the pretense that this meeting was entirely her idea. She was incredibly affectionate when she wanted to be, but she spent most of her time in solitude. Like so many of her ilk, she loved to perch herself on or a near a windowsill, surveying the outdoors for hours. It strikes me now how quintessentially feline that behavior is: a docile carnivore balanced on the border of a human home, alone and content, yet with all its senses tuned to the world beyond.

I have spent a lot of time watching the animals in my backyard these past few months. Every morning, honeybees descend to inspect the creeping charlie’s tiny purple trumpets. The same three squirrels leap from tree to ground each afternoon, secreting away nuts and other morsels. Now and then, robins perch on the fence, staring at me with anxious, obsidian eyes. It is easy to assume that all these creatures, with their daily routines, perceive the passing of time the same way I do. Something happens—a leaf falls, a butterfly flaps its wings—and it happens at the same rate for all observers in the garden. In truth, time may pass at quite different rates for different kinds of living things.
Although we can never directly experience another person or animal’s mental state, we can use the objective to make inferences about the subjective. Humans and other creatures generally construct a sense of time by recording changes in their environment with their sensory organs. Each of those organs—every eye, ear, and antenna—has testable limits. One such limit is known as the critical flicker frequency (C.F.F.): the threshold at which a light is flickering so quickly that an animal can no longer distinguish the discrete flashes and instead sees only an unwavering glow. Movies are series of still images played quickly enough to create the illusion of seamless motion in real time. Play them too slowly—too far below the human flicker frequency—and you get a slide show.
To determine the C.F.F. of a particular creature, scientists train animals to behave one way when they see a pulsing light and another when they see a continuous gleam. They might, for example, teach chickens to peck a glimmering L.E.D. and to resist pecking an unchanging one, in order to get their beaks on bits of boiled spaghetti (a very alluring treat, as far as chickens are concerned). Increasing the rate of flickering from one trial to the next reveals the point at which the light is flashing too quickly for an animal to differentiate it from a steady beam.
Last year, Andrew Jackson and Kevin Healy, of Trinity College Dublin, along with several collaborators, decided to compare the C.F.F.s of a wide range of species. It seems like an obvious thing to do, and yet no one had ever conducted such a comprehensive analysis. “I think it’s really easy to get locked into the one species you study,” Jackson says. “Sometimes we forget to step back and look at the big picture.” He was inspired to do so at a train station in Dublin, watching kids scamper around their parents’ legs. The young and the old seemed to be living at very different tempos. For both physiological and psychological reasons, a given period of time often seems to last longer to a child than to an adult. To a young child, inexperienced and hungry for knowledge, each new moment is so pregnant with possibility that it expands as it falls upon the mind like a plop of ink in water. For comparatively jaded adults, most new moments are just another drop in the bucket, though the rate at which they seem to fall can still slow and quicken considerably depending on one’s emotional state. If such differences exist between people, how much might time perception vary in the entire animal kingdom?
A lot. If life is a movie, the frame rate fluctuates widely from one species to another. Jackson focused on vertebrates. At one end of the spectrum he uncovered was the golden-mantled ground squirrel, which can see a hundred and twenty flashes a second. At the other was the European eel, with a C.F.F. of just fourteen, and the leatherback sea turtle, at fifteen. Humans are more or less in the middle, at sixty. Dogs: eighty. Rats: thirty-nine. Add invertebrates to the list and things get even wilder. Booralana tricarinata, a deep-sea cousin of the roly-poly, boasts the lowest known C.F.F.: four flashes a second. Its view of life may be something like a fitful and grainy stop-motion film. The highest recorded C.F.F.s belong to flies, some of which can see around two hundred and fifty flashes a second—four times better than our ability to detect changes in our visual field. Of course, a fly’s vision is, on the whole, much blurrier and more fragmented than our own. But if we could adopt the perspective of a fly—and preserve the contiguity and crispness of our own vision—everything around us would probably appear to glide along in Matrix-style bullet time.
These discrepancies exist because the eyes of different organisms have different shutter speeds. When photons strike light-sensitive neurons in the eye, they kick-start a complex chain of chemical reactions and electrical impulses that travel to the brain, and culminate in an image of a particular moment in time. To take the next snapshot, neurons in the eye and the brain must essentially reset themselves. The higher an animal’s C.F.F., the swifter its neurons complete this rebooting.
Jackson and his team concluded that small animals with speedy metabolisms generally have higher C.F.F.s than larger animals with more sluggish physiology. A faster metabolism means greater potential for quick-firing neurons. Not all scientists are ready to accept that this is a true statistical trend, though. “I think the general basis of argument is correct, but it could be strengthened by adding many more data points,” Simon Laughlin, a neuroscientist and zoologist at the University of Cambridge, says. Even more important than body size and metabolism may be the idiosyncrasies of an animal’s lifestyle and its particular ecological role. Grazers on the savannah are many times more massive than insects, but fine temporal resolution would be a big help for escaping lions and cheetahs. In contrast, a high C.F.F. may not be as advantageous for the unhurried tortoise, which can retreat into built-in armor when threatened rather than scurry away.
Considering how an animal’s lifestyle defines its view of the world also helps us identify that historic moment when animals shifted from merely perceiving the present to contemplating the past and the future. Five hundred million years ago, every animal in existence made its permanent home in the ocean. We can safely assume that many such animals experienced time as one moment following the next and, if they were like many modern finned and tentacled denizens of the deep, formed memories. Indeed, the octopus, one of the ocean’s greatest geniuses, seems to have evolved the means to use tools, solve problems, and possess foresight completely independently of land animals. But mental time travel—the ability to consciously relive past experiences and imagine the future—may have really taken off at the water’s edge.
This notion forms part of the buena vista hypothesis, which Malcolm MacIver, of Northwestern University, conceived to explain the origins of consciousness: In deep or murky water, animals can see only a few metres ahead of themselves at any moment. There is not much opportunity to make long-term plans if you cannot even glimpse where you will be in a few minutes. Around three hundred and fifty million years ago, however, when pseudo-limbed fish first crawled ashore and brought their eyes into the open air, they could suddenly see much farther into the distance than ever before. With such grand views, they could learn much more about the world from a single glance and construct more intricate mental maps of the surrounding landscape, which in turn permitted more sophisticated thought and behavior. If one of these early land cruisers spotted several potential meals in the distance, for example, it could traverse its greatly expanded mental map, pondering not only which prey to attack but also when to make its move—a strategic luxury that would have been much rarer in the water. In other words, our evolutionary ancestors may have been limited in their ability to mentally travel through time until they got to the right place.

In 1990, while visiting a research camp in central Borneo, the primatologist Anne Russon saw an orangutan nicknamed Supinah attempt to make fire. Supinah sauntered toward an ashy fire pit, picked up a stick glowing with embers, and dipped it into a nearby cup full of liquid. Russon thought that the cup contained water, but it in fact held kerosene. Fortunately, that bath did little more than dampen the wood. Yet Supinah persisted: she got a second glowing stick, blew on it, fanned it with her hands, and rubbed it against other sticks. She never got the right steps in the right order to start a fire, but what foiled her was not her innate intelligence. She had a clear goal in mind and the right kind of brain to achieve it. She just needed a little more practice.
At the time, Russon was visiting Camp Leakey, which the anthropologist Biruté Galdikas established, in 1971, to study orangutans, just as Jane Goodall and Dian Fossey had done, in Africa, to observe chimpanzees and gorillas, respectively. Since then, Galdikas, Russon, and a handful of other orangutan specialists have learned firsthand just how intelligent and resourceful the animals really are. Some of their mental skills may exceed those of their great-ape brethren. Michelle Desilets, executive director of the Orangutan Land Trust, has summarized the unique intellect of orangutans like this: “They say that if you give a chimpanzee a screwdriver, he’ll break it; if you give a gorilla a screwdriver, he’ll toss it over his shoulder; but if you give an orangutan a screwdriver, he’ll open up his cage and walk away.”
Compared with chimpanzees, which are highly excitable, orangutans seem far more sober and considerate. They move deliberately and often spend a good deal of time silently watching before deciding how to act. At Camp Leakey, the orangutans had plenty of opportunity to observe and imitate people. They soon developed a habit of stealing canoes, paddling them downriver, and abandoning them at their destinations. Even triple and quadruple knots in the ropes securing the canoes to the dock did not deter the apes. Over the years, they have also learned to brush their teeth, bathe themselves, wash clothes, weed pathways, wield saws and hammers, and soak rags in water in order to cool their foreheads with them. And they have done all of this without any instruction.
For a time, the deftness of orangutans living near people was puzzling, because they did not seem to display such acumen on their own in the wild. More careful observation revealed otherwise. Scientists now know that wild orangutans use sticks to search for ants under tree bark, make hats and umbrellas out of large leaves, and sometimes drape themselves with lianas—forest vines—as if donning necklaces. When they need to cross a river that is too deep to ford safely, some orangutans bend saplings into bridges and twist several tree-anchored vines into a rope for extra support.
In 2004, Russon and her colleagues, along with a crew from Animal Planet, began documenting another intriguing behavior. That year, Russon saw a young male orangutan pick up and eat a dying catfish stranded on the bank of a dwindling river. By 2007, Russon and Animal Planet had noted forty other instances of similar behavior. The apes pulled fish out of shallow ponds by their tails or pursued them into deeper water. One orangutan even attempted to skewer a fish with a large stick, scaring it onto land. Although baboons and other primates have been observed catching fish, these are the only great apes currently known to systematically hunt and eat fish.
In orangutans—as in chimpanzees, dolphins, and a few other species—a penchant for tool use and problem-solving has brought about cultural learning, in which animals teach one another new behaviors, transmitting the information among members of a group, sometimes from one generation to the next. Culture is what gives different groups of the same species distinct ways of achieving the same goal. In one experiment, conducted at a rehabilitation center in Sumatra, researchers found that nine of the thirteen orangutans from swampy regions of the island knew how to use a stick to extract honey from a hole in a log. By contrast, only two of the ten from coastal regions could do the same—the only two that had been living at the rehabilitation center long enough to learn the trick from the swamp natives.
At first, the idea of widespread social learning among orangutans was at odds with traditional depictions of the ginger apes as loners. Researchers had long assumed that the mother-infant pair was the only unit of orangutan social structure. But it turns out that adult female relatives stick together: they have overlapping ranges and periodically interact. “I grew up in rural Saskatchewan,” Russon, who now works and teaches at York University, in Toronto, told me. “And, for me, that is exactly what orangutan social life is like. There are communities, but they are very broadly dispersed. It might be fifteen miles to your cousin’s place, or another twenty miles to the next nearest relative, but everybody knows everybody.” Adolescent orangutans—curious and audacious—regularly make new friends. These wandering youngsters, vaulting from one tree to the next, are likely the torchbearers of orangutan culture.
If adolescence is a period of great intellectual discovery for orangutans, it is also a time of acute vulnerability. Poachers routinely kill protective orangutan mothers, kidnap the juveniles, and sell them to disreputable zoos or traders on the illegal pet market. Such abductions have long been a serious threat to orangutans, but even more pressing is the wanton destruction of their habitat. Orangutans once lived throughout Southeast Asia. Today, they are restricted to the islands of Borneo and Sumatra. The rainforests they inhabit are constantly harvested for timber or slashed and burned to make way for palm plantations. Among the slowest of the apes, orangutans are often trapped in the inferno.
At the turn of the twentieth century, wild orangutans numbered perhaps three hundred thousand strong. The International Union for Conservation of Nature, citing data from the early and mid-two-thousands, says that Borneo’s present orangutan population consists of between forty-five thousand and sixty-nine thousand individuals, and that Sumatra has a mere seventy-three hundred. Considering that habitat destruction has been especially rampant in the past decade, however, the Orangutan Conservancy estimates that there are only forty thousand wild orangutans left alive today.
Clear evidence of sentience is by no means the only reason to rescue a creature from extinction. Still, the fact that orangutans are self-aware, that they are capable of reason and cultural learning, that they possess a manual dexterity and an intelligence undeniably similar to our own, makes the way we have treated them all the more abominable. The word “orangutan” comes from a seventeenth-century Malay expression meaning “forest person.” Perhaps it’s time we learned again to see the person in the ape.

For most of my life, I have been severely prejudiced against the potato. Like many Americans, I regarded potatoes as among the most banal of vegetables. They were clearly less interesting than other tubers and roots, lacking the lip-staining intensity of the beet, the bright color and crunch of the carrot, the surprising heat of the radish. Potatoes were just generic lumps of starch best used as couriers for salt, fat and ketchup.

But that was before I went to Peru.

Although most people associate the potato with Ireland and Idaho, its evolutionary birthplace is Peru, a country that, to this day, has far more varieties of the vegetable than anywhere else. Having read about this diversity, I traveled to an annual food festival in Lima known as Mistura, where I encountered tables heaping with all manner of bizarre and beautiful potatoes: spuds that were shaped like croissants and cucumbers; that were knobbed, ruffled, and bowed; whose skin and flesh were as yellow as egg yolk, as black as ink, or splotched and striped with shades of red, blue and violet.

The next day, back at the apartment where I was staying, I lifted the lid off a pot of boiling potatoes. Pungent ribbons of steam curled their way into my nose: the smell of newly dried cornstalks and wet earth mingled with something much sweeter, like that first bite of perfectly ripe peach. A tan one decorated with swirls of cherry tasted distinctly of roasted chestnuts. Another, with deep purple skin and white flesh marbled pink, had the flavor of caramelized beets. One daffodil yellow potato was unbelievably buttery. Many were more dense and filling than I expected.

Savoring this meal, I thought about how utterly different it was from all my experiences with potatoes in the U.S. Although farmers markets sometimes offer fingerlings and purple potatoes, there are only two specific varieties familiar to the average American: the Russet Burbank, which makes up the vast majority of potatoes grown in the U.S., and the Yukon Gold. The other potatoes commonly seen in supermarkets are almost never called by their proper names; we simply refer to them as small or round reds and whites. Americans likewise dismiss the highly varied textures and innate flavors of potatoes. Instead, we prefer to smother them in lab-concocted powders. In the chip world, longtime favorites such as sea salt and vinegar have recently started to share shelf space with cappuccino, chicken and waffle, cheesy garlic bread and General Tso’s.

From our partners at VICE
Watch: Fungus: The Plastic of the Future
 
It’s easy to laugh off these processed peculiarities, but America’s addiction to fries and chips, our pitifully limited selection of potatoes in the supermarket — our whole attitude towards the potato — are symptoms of something more insidious. Together, the rise of fast food chains and the pressures of industrial agriculture have enforced a bleak homogeneity on much of modern produce. By prioritizing comfort, cheapness and convenience above all else, we have sacrificed a lot of our food’s flavor, diversity and inherent nutritional value, not to mention the toll it’s taken on the pleasure of eating. Among all vegetables, the potato has likely suffered the most.

By prioritizing comfort, cheapness and convenience above all else, we have sacrificed a lot of our food’s flavor, diversity and inherent nutritional value.
It doesn’t have to be this way. Around the country, determined plant breeders, farmers, and chefs are joining forces to reinvent the potato and the transform the way Americans perceive the versatile tuber. Consider what happened to apples: By the 1980s, Americans were so fed up with the dominant and inaptly named Red Delicious that all kinds of tastier varieties soared in popularity. Today, people expect mainstream grocery stores to offer a wide range of apples: Gala, Fuji, Honeycrisp, Pink Lady, Braeburn and Jonagold, to name a few. The potato’s champions want to bring this same kind of diversity to the humble spud. “We’re definitely seeing a trend towards more diversity,” says Shelley Jansky, a longtime potato breeder at the University of Wisconsin-Madison. “I would like all potato varieties marketed by name, people eating more fresh potatoes, thinking about flavor and texture, and realizing that not all potatoes are the same.”

Jansky and likeminded potato enthusiasts envision a revitalized American potato market that is much closer to Peru’s cornucopia. Several potatoes they have created through breeding or resurrected from seed banks are already available at some farmers markets, restaurants, and grocery stores, and several more should make their debuts soon.

***
The relationship between potatoes and their human cultivators is long and tumultuous. The Inca’s ancestors first domesticated the wild potato between 7,000 and 10,000 years ago on the border of Peru and Bolivia. By 4,000 years ago, the potato had become a staple crop for native Andeans. The tuber was so integral to their culture that some groups based units of time on how long it took to cook various types.

Back then, the potato was synonymous with diversity. The Andeans inhabited a mountainous mosaic of microclimates in which one plot of land presented a very different set of growing conditions than its neighbor. No single variety could survive in such a heterogeneous landscape, so the Andeans diversified — to the extreme. Farming so many different types of potatoes also provided a more interesting and enjoyable diet, a tradition that is still alive today. “If you go to a typical Andean household,” explains Stef de Haan, a researcher at the International Potato Center in Lima, “they will eat what is called chajru, which means ‘mixture’ in the Quechua language. They sit around a big bowl of potatoes. And the joy of eating those, the culinary delight, is that every time you pick a potato, you pick a different one. In Quechua, especially when it comes to the taste of potatoes, they have this whole unique vocabulary — almost like somebody from France would tell you about the taste of wine.”

More Potato Stories

Cropped: Potatoes

Consider The Salt-Tolerant Potato

McDonald's Refuses To Buy GM Potatoes For Its Fries
Around 1562, the potato traveled with European explorers from South America to the Canary Islands and subsequently to Spain. From there, it hitched a ride with herbalists and farmers to Italy, England and the Low Countries. Europe’s acceptance of the potato as something fit to eat was staggered and piecemeal. Botanists, intrigued by the unfamiliar vegetable, began gossiping. Their rumors turned into widespread myths that the potato caused wind and leprosy. People came to regard potatoes as poisonous, difficult to digest, and suitable only for pigs. One European country was the great exception, however: Ireland, where peasants were struggling to find anything that would grow in the only soil the English had left them.

Then came along French army pharmacist Antoine-Augustin Parmentier, the Johnny Appleseed of the potato, as historian Charles C. Mann put it. During the Seven Years War, the Prussians repeatedly imprisoned Parmentier, feeding him little but potatoes. Yet, to his astonishment, he rather enjoyed his meals and remained in good health. After the war ended, amid revolts in France over the increasing price of bread, Parmentier began to promulgate the nutritional virtues of the potato and promote the tuber as the ideal alternative to grains. He invited Benjamin Franklin and other luminaries to lavish meals featuring potatoes prepared every which way: in stews, salads, pies and breads. He presented Louis XIV with a bouquet of starlike potato flowers; on a whim, the king slipped one through his buttonhole, inspiring a fad of potato flower accessorizing among the French royalty. Perhaps most cunningly, Parmentier arranged for soldiers to stand guard along a field of potatoes at the edge of Paris — so that the public would perceive the crop as valuable — but dismissed the soldiers at night, so people would have a chance to raid the field.

Thanks in part to Parmentier, Europe learned to recognize the potato’s nutritional value and adopted it as a solution to widespread famine. Potatoes were easy to sow and harvest, as well as inherently more productive than grains: wheat, rice, corn and barley could only grow so tall and heavy before they fell over; nestled in the ground, tubers could swell to impressive sizes, yielding two to four times more calories per acre. A potato of average size contains 115 calories, 3.2 grams of protein, plenty of phosphorous and iron and half as much vitamin C as an orange.

European ships brought potatoes to the East Coast of North America in 1621. Instead of allowing some potato plants to flower, cross-pollinate and produce seeds — each of which is a tiny packet of genetic diversity — America’s earliest potato breeders opted for the easier method of cloning, slicing off bits of tuber to plant in the ground. By 1850, America’s cloned spuds were feeble and riddled with virus-borne diseases. In response, breeders began to import potatoes from South America, creating new cultivars that not only were more resilient, but also had a variety of unique flavors, textures and shapes. Crack open a copy of M.M. Vilmorin-Andrieux’s 19th-century text The Vegetable Garden and you can sample some of this diversity, which is reminiscent of modern Peru: page after page of alien potatoes with canary yellow flesh and midnight blue skin; with casings “split like the skin of a truffle” or cratered with deeply sunken eyes; with names like snowflake, Manhattan, Early Rose, walnut-leaved kidney potato and red-skinned flour ball.

During this time, a New York Episcopalian minister named Chauncey E. Goodrich — who grew potatoes as a hobby — ordered some from the American consulate in Panama, including a cultivar known as Rough Purple Chili: the progenitor of the Russet Burbank. In the early 1900s, the Russet Burbank was just one of thousands of different types of potatoes grown in the U.S. By the late 1980s this single variety made up the vast majority of U.S.-grown potatoes and is still the most familiar potato in the country today. Why? Well, it’s an all-purpose potato that holds up all right — though by no means ideally — whether baked, boiled, or mashed. But its most common and popular form is also the main reason for its rise to dominance: the McDonald’s French fry.

In order to make those crisp golden fries, McDonald’s and other French fry producers need a very particular kind of potato.
In order to make those crisp golden fries, McDonald’s and other French fry producers need a very particular kind of potato. It must be unusually long and smooth; pale and low in sugar, so that it does not brown when cooked; relatively dry; and hardy so it can be shipped great distances and stored for a long time. The Russet Burbank satisfied all these requirements better than any other variety: even after 12 months of storage, a Russet will yield plenty of gorgeous fries. Following World War II, new technologies greatly improved the efficiency of modern potato processing factories, which provide burger joints with frozen fries and also turn spuds into chips, hash browns, starch and flour. They, too, insisted on white potatoes uniform in shape and durable enough to travel from coast to coast; flavor, color, and diversity were not of great concern. In fact, varieties developed for potato chip companies (such as the Frito-Lay potato) are so high in starch and so low in sugar that they never appear in supermarkets; if cooked at home, they would taste awful.

The demands of potato processors, and the apotheosis of the Russet Burbank, revolutionized not just the way Americans grow and eat potatoes, but also our entire concept of the vegetable. Breeders started to make many fresh-market potatoes more Russet-like and farmers started growing even more Russets than before; it was easier to farm a single highly lucrative variety for both processors and the fresh market than to bother with many different kinds of spuds for the produce aisle and all their different growing requirements. In 1960 the average American ate 81 pounds of fresh potatoes a year; by the 2000s, that number had nearly halved. In 2013, the U.S. harvested 43.5 billion pounds of potatoes. About 63 percent was used for processing of some kind. Only 24 percent went to the fresh market. The outcome is a kind of collective amnesia: we have forgotten what the potato can be — what it really is. What we need are a few modern-day Parmentiers, tireless champions of the lowly spud, to lead the potato renaissance.

***
John Mishanec does not typically get his potatoes from the supermarket. A jovial man with a half laurel wreath of flossy white hair, Mishanec prefers to buy in bulk straight from the source — a 50-pound bag or two from his favorite farmers.

At age 26, Mishanec began working for Cornell University as an integrated pest management specialist: someone who teaches farmers how to keep insects at bay while using as few toxic chemicals as possible. Over the past 40 years, he has trained dozens of onion, sweet corn and potato farmers throughout New York, with a special passion for the latter: “I definitely have a fondness, an affinity for potatoes. It goes a long way back.” When Mishanec was growing up in Olean, New York, his father taught himself how to be a potato breeder. Everyone in the neighborhood knew about the Mishanec’s bountiful potato patch and the odd collection of tubers and seeds in their attic.

‘We evaluated how different varieties performed in the kitchen. The chefs were fascinated.’
Seven years ago, Mishanec wrote a grant proposal to bridge the traditionally segregated worlds of the potato breeder and professional cook. He was inspired by breeders and farmers who wanted to revamp the potato market — to start selling them by name and culinary appeal, as had happened for apples. So he visited all the culinary schools he could find in New York, bringing them truckloads of colorful and flavorful potatoes grown by a handful of the state’s farmers. “We evaluated how different varieties performed in the kitchen. The chefs were fascinated. They really loved the differences: that one type of potato could look just like another, but perform differently.”

The culmination of Mishanec’s efforts is a one-of-a-kind guide to cooking potatoes that details the appearance, texture and flavor of 17 different varieties (with names like Adirondack red and blue, Chieftain, Keuka gold, Red Norland, Superior, Eva, Lehigh) and breaks down how the spuds hold up when baked, boiled, fried, mashed or turned into salad. The Culinary Institute of America, Monroe College, the State University of New York College of Agriculture and Technology at Cobleskill, and other organizations rely on this guide today. “It was so great that someone like John took the initiative to link the culinary community and agricultural community,” says Anne Rogan, an instructor at Cobleskill, who built upon Mishanec’s work by using the potato to teach students how to conduct professional taste tests. “People are surprised that I would use something so inexpensive and readily available for sensory science, but I think the potato should get a lot more recognition than it does now. The potato’s flavors and textures are so lovely and complicated. Most people aren’t stopping to really taste potatoes.”

Shelley Jansky is. This past October, 900 miles from New York in Verona, Wisconsin, Jansky took a confident stance before a room of farmers, restaurateurs, and curious locals. “It’s odd that as potato breeders we seem to focus on everything but flavor,” she said. “People are getting more interested in it these past few years. You hear a lot more talk about it. Trouble is, we still don’t have a good handle on exactly what accounts for potato flavor.”

‘It’s odd that as potato breeders we seem to focus on everything but flavor.’
Jansky did not grow up dreaming about studying potatoes; her love for them flowered from an arranged marriage — this was the vegetable assigned to her when she began her professorship. “You work on something for 30 years and you develop a passion for it,” she says. “I’m convinced the potato is a good food, a healthy, nutritious, inexpensive food.” And a flavorful one. Jansky is determined to bring flavor to the forefront of her profession. She and her team have been rounding up cooks, farmers and members of the general public for a series of potato taste tests that examine acidity, sweetness, saltiness, bitterness, texture, off-flavors, and overall flavor intensity. Like Mishanec, they also routinely deliver spuds to a contingent of chefs for a hands-on culinary evaluation. Once they figure out which ones taste best — as well as how they stand up to pests and diseases — the team will cross-pollinate the most promising varieties to create a new generation of resilient and delicious spuds.

Jansky is not content to codify subjective experience through taste tests, though. She wants to understand the empirical nitty gritty of potato taste, right down to the level of molecules. A few years ago, after collecting every relevant study she could find, Jansky published the definitive account of potato flavor chemistry. There’s not nearly as much data as she would like, but what she discovered is fascinating. Unlike fruits, which evolved to attract hungry animals that inadvertently help spread a plant’s seeds, wild potatoes were more interested in fending off nosy visitors. Any inherently appetizing flavors and aromas in wild tubers were an accident of chemistry, with which the ancient Andeans tinkered through millennia of breeding. Wild potatoes are often rife with toxic and bitter compounds known as glycoalkaloids, but in small enough doses they actually contribute to a pleasant taste.

Cooking a potato completely changes its flavor chemistry, often in delicious ways. A cooked spud has some of the highest levels of umami compounds — molecules that stimulate a “pleasant savory taste”—of any plant food. At least 228 distinct aromatic molecules contribute to a cooked potato’s flavor, but most are poorly studied.

To help fill in the gaps in the data, one of Jansky’s graduate students is currently creating detailed molecular flavor profiles of many different potatoes grown for the fresh market. She is also writing a grant proposal to identify the genes responsible for high and low starch levels. Similarly, Mishanec’s colleague Walter De Jong has pinpointed the genes responsible for red and purple skin and yellow flesh, as well as some of the genes that control whether these pigments saturate or merely speckle a spud.

Potatoes sometimes get a bad rap nutrition-wise, but there’s nothing inherently unhealthy about a baked potato with a little salt; it’s the way we process and eat them that can clog arteries and pack on pounds. More flavorful and colorful potatoes will encourage healthier ways of enjoying them (in fact, it is antioxidants and vitamins that give potatoes their colors). Pinpointing color, flavor and texture genes will allow potato breeders to look for them in the DNA of potato seeds and young potato plants, which in turn helps them make smarter and more efficient crosses that enhance those traits — something breeders have already done for strawberries and tomatoes. This approach, known as marker-assisted selection, is an increasingly popular alternative to far more controversial genetic engineering. One of De Jong’s most recent creations, which came out of his studies on color, is Violeta: a spud whose yellow flesh and purple-stippled skin have caught the eye of famed New York chef Dan Barber.

As tastemakers, chefs are powerful allies for breeders hoping to introduce new vegetables to the public. The Yukon Gold was first developed in Ontario in the 1960s, using yellow potatoes imported from Peru, and commercially released in the 1980s. It really took off in the 1990s, when restaurants in California recognized its appeal. To bring a whole new line of new potatoes out of the field and to the public, however, potato breeders will also have to convince large-scale growers and distributors that it’s worthwhile to experiment. There’s long been resistance to such innovation, but attitudes are starting to change.

As tastemakers, chefs are powerful allies for breeders hoping to introduce new vegetables to the public.
David Fairbourn is the manager of industry communications and policy for the U.S. Potato Board, an organization whose mission is to increase demand for potatoes and whose employees hold the vegetable in adorably high esteem. (“The potato is the one vegetable that can be served breakfast, lunch, and dinner,” Fairbourn says. “We are very thrilled about that. We know there’s lots of vegetables out there that would like to have that distinction.”) The board’s consumer research confirms that although Russets, Yukons, and small red potatoes are the most commonly purchased by far, more unique varieties are steadily gaining popularity. “You can now find bags of fingerlings and baby-sized creamers in microwave bags,” Fairbourn says. “More and more growers and retailers are becoming interested in these types.”

The Idaho-based J.R. Simplot Company — which pioneered the mass production of frozen French fries and is the primary supplier for McDonald’s — has been meeting with Jansky to discuss trying tastier and more variedly textured potatoes for both processing and the fresh market. Given the backlash against processed food, they might be on the lookout for a much healthier alternative to standard deep-fried French fries: say, baked wedges of potatoes packed with innate flavor. “They want to get people excited about new varieties,” Jansky says. “If you could make a potato’s natural taste stronger, taste more potato-y, people would say, ‘Yes, when I put it on a plate with a little salt, it’s perfect.’ I think that’s why the industry is interested.”

“I think diversification is starting to happen,” says John Norguard, an agronomist at Black Gold Farms, one of the country’s leading potato growers with operations in 11 states. “There is definitely more interest today than there was 5 or 10 years ago in different sizes and colors, gourmet and specialty varieties.” Sam’s Club, Walmart and few other stores have recently started offering Albert Bartlett Roosters—a nutty potato that starts pink and roasts golden brown—and Blonde Bellas, which have a “sweet, buttercream taste.” Wada Farms — another giant potato grower — has developed something not unlike the Quechua’s chajru: bags of bite-size multicolored round potatoes that can be cooked in their peels.

At the end of October, I joined Jansky and her colleagues Julie Dawson and Ruth Genger at an agricultural research station in Wisconsin for one of their potato showcases. On an outdoor bench, beside rows of lettuces, herbs, and onions, Genger and her assistants arranged woven green baskets full of unusual potatoes. Barbara was a plump yellow spud graffitied with sprays of purple. When sliced open, Elmer’s Blue resembled a cross-section of an amethyst geode. In the left corner was a potato that looked nearly identical to one I had tried in Peru — the one that tasted like roasted chestnuts. To the right was a brand new cultivar with salmon skin and rosy interior: a cross between the red-skinned, white-fleshed Chieftain and Purple Majesty. Around me, farmers, gardeners, and locals oohed and ahed, snapping photos and gingerly touching the spuds, as though they were the fragile eggs of some exotic creature. It really was a marvelous spread: a tableau of everything we have forgotten about potatoes and everything that is on its way back.
